{
  "url": "https://api.github.com/repos/JuliaLang/julia/issues/60278",
  "repository_url": "https://api.github.com/repos/JuliaLang/julia",
  "labels_url": "https://api.github.com/repos/JuliaLang/julia/issues/60278/labels{/name}",
  "comments_url": "https://api.github.com/repos/JuliaLang/julia/issues/60278/comments",
  "events_url": "https://api.github.com/repos/JuliaLang/julia/issues/60278/events",
  "html_url": "https://github.com/JuliaLang/julia/pull/60278",
  "id": 3675923564,
  "node_id": "PR_kwDOABkWpM62FsZG",
  "number": 60278,
  "title": "NFC: tidy up the stock GC codebase (part 1)",
  "user": {
    "login": "d-netto",
    "id": 61364108,
    "node_id": "MDQ6VXNlcjYxMzY0MTA4",
    "avatar_url": "https://avatars.githubusercontent.com/u/61364108?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/d-netto",
    "html_url": "https://github.com/d-netto",
    "followers_url": "https://api.github.com/users/d-netto/followers",
    "following_url": "https://api.github.com/users/d-netto/following{/other_user}",
    "gists_url": "https://api.github.com/users/d-netto/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/d-netto/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/d-netto/subscriptions",
    "organizations_url": "https://api.github.com/users/d-netto/orgs",
    "repos_url": "https://api.github.com/users/d-netto/repos",
    "events_url": "https://api.github.com/users/d-netto/events{/privacy}",
    "received_events_url": "https://api.github.com/users/d-netto/received_events",
    "type": "User",
    "user_view_type": "public",
    "site_admin": false
  },
  "labels": [
    {
      "id": 195227131,
      "node_id": "MDU6TGFiZWwxOTUyMjcxMzE=",
      "url": "https://api.github.com/repos/JuliaLang/julia/labels/GC",
      "name": "GC",
      "color": "2CD3F0",
      "default": false,
      "description": "Garbage collector"
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [],
  "milestone": null,
  "comments": 1,
  "created_at": "2025-11-28T21:00:28Z",
  "updated_at": "2025-11-29T02:56:45Z",
  "closed_at": "2025-11-29T02:56:43Z",
  "author_association": "MEMBER",
  "type": null,
  "active_lock_reason": null,
  "draft": false,
  "pull_request": {
    "url": "https://api.github.com/repos/JuliaLang/julia/pulls/60278",
    "html_url": "https://github.com/JuliaLang/julia/pull/60278",
    "diff_url": "https://github.com/JuliaLang/julia/pull/60278.diff",
    "patch_url": "https://github.com/JuliaLang/julia/pull/60278.patch",
    "merged_at": "2025-11-29T02:56:43Z"
  },
  "body": "Non-functional changes to improve the readability of the stock GC code:\r\n- Add comments and ASCII diagrams documenting the layout of pool-allocated pages and the multi-level page table containing page allocation status.\r\n- Replace `static inline` with `STATIC_INLINE` for consistency with the rest of the GC.\r\n- Move or remove a few functions for better file encapsulation and organization.\r\n- Delete unused variables from GC headers.\r\n- Replace nested conditional statements with guarded clauses to improve readability.",
  "reactions": {
    "url": "https://api.github.com/repos/JuliaLang/julia/issues/60278/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/JuliaLang/julia/issues/60278/timeline",
  "performed_via_github_app": null,
  "state_reason": null,
  "score": 1.0,
  "files": [
    {
      "sha": "4a489d3e276bcc741a7416edb4f8f2e71be935a8",
      "filename": "src/gc-common.h",
      "status": "modified",
      "additions": 1,
      "deletions": 0,
      "changes": 1,
      "blob_url": "https://github.com/JuliaLang/julia/blob/c4901df9a70f731acec3d0e852658d7bd0bb8e6f/src%2Fgc-common.h",
      "raw_url": "https://github.com/JuliaLang/julia/raw/c4901df9a70f731acec3d0e852658d7bd0bb8e6f/src%2Fgc-common.h",
      "contents_url": "https://api.github.com/repos/JuliaLang/julia/contents/src%2Fgc-common.h?ref=c4901df9a70f731acec3d0e852658d7bd0bb8e6f",
      "patch": "@@ -28,6 +28,7 @@ extern \"C\" {\n // GC Big objects\n // =========================================================================== //\n \n+// layout for big (>2k) objects\n JL_EXTENSION typedef struct _bigval_t {\n     struct _bigval_t *next;\n     struct _bigval_t *prev;"
    },
    {
      "sha": "9741d08504afcc22533c6ca147eea6b403ca48bd",
      "filename": "src/gc-debug.c",
      "status": "modified",
      "additions": 0,
      "deletions": 21,
      "changes": 21,
      "blob_url": "https://github.com/JuliaLang/julia/blob/c4901df9a70f731acec3d0e852658d7bd0bb8e6f/src%2Fgc-debug.c",
      "raw_url": "https://github.com/JuliaLang/julia/raw/c4901df9a70f731acec3d0e852658d7bd0bb8e6f/src%2Fgc-debug.c",
      "contents_url": "https://api.github.com/repos/JuliaLang/julia/contents/src%2Fgc-debug.c?ref=c4901df9a70f731acec3d0e852658d7bd0bb8e6f",
      "patch": "@@ -1100,27 +1100,6 @@ void gc_count_pool(void)\n     jl_safe_printf(\"************************\\n\");\n }\n \n-void _report_gc_finished(uint64_t pause, uint64_t freed, int full, int recollect, int64_t live_bytes) JL_NOTSAFEPOINT {\n-    if (!gc_logging_enabled) {\n-        return;\n-    }\n-    jl_safe_printf(\"\\nGC: pause %.2fms. collected %fMB. %s %s\\n\",\n-        pause/1e6, freed/(double)(1<<20),\n-        full ? \"full\" : \"incr\",\n-        recollect ? \"recollect\" : \"\"\n-    );\n-\n-    jl_safe_printf(\"Heap stats: bytes_mapped %.2f MB, bytes_resident %.2f MB,\\nheap_size %.2f MB, heap_target %.2f MB, Fragmentation %.3f\\n\",\n-        jl_atomic_load_relaxed(&gc_heap_stats.bytes_mapped)/(double)(1<<20),\n-        jl_atomic_load_relaxed(&gc_heap_stats.bytes_resident)/(double)(1<<20),\n-        // live_bytes/(double)(1<<20), live byes tracking is not accurate.\n-        jl_atomic_load_relaxed(&gc_heap_stats.heap_size)/(double)(1<<20),\n-        jl_atomic_load_relaxed(&gc_heap_stats.heap_target)/(double)(1<<20),\n-        (double)live_bytes/(double)jl_atomic_load_relaxed(&gc_heap_stats.heap_size)\n-    );\n-    // Should fragmentation use bytes_resident instead of heap_size?\n-}\n-\n #ifdef __cplusplus\n }\n #endif"
    },
    {
      "sha": "faee7a60fc40673b8d5411e2fd9dbe7f3c3078f9",
      "filename": "src/gc-pages.c",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/JuliaLang/julia/blob/c4901df9a70f731acec3d0e852658d7bd0bb8e6f/src%2Fgc-pages.c",
      "raw_url": "https://github.com/JuliaLang/julia/raw/c4901df9a70f731acec3d0e852658d7bd0bb8e6f/src%2Fgc-pages.c",
      "contents_url": "https://api.github.com/repos/JuliaLang/julia/contents/src%2Fgc-pages.c?ref=c4901df9a70f731acec3d0e852658d7bd0bb8e6f",
      "patch": "@@ -165,7 +165,7 @@ NOINLINE jl_gc_pagemeta_t *jl_gc_alloc_page(void) JL_NOTSAFEPOINT\n }\n \n // return a page to the freemap allocator\n-void jl_gc_free_page(jl_gc_pagemeta_t *pg) JL_NOTSAFEPOINT\n+NOINLINE void jl_gc_free_page(jl_gc_pagemeta_t *pg) JL_NOTSAFEPOINT\n {\n     void *p = pg->data;\n     gc_alloc_map_set((char*)p, GC_PAGE_FREED);"
    },
    {
      "sha": "8a9760ca5033bc289b1bc956b978c2297dc084ae",
      "filename": "src/gc-stock.c",
      "status": "modified",
      "additions": 213,
      "deletions": 181,
      "changes": 394,
      "blob_url": "https://github.com/JuliaLang/julia/blob/c4901df9a70f731acec3d0e852658d7bd0bb8e6f/src%2Fgc-stock.c",
      "raw_url": "https://github.com/JuliaLang/julia/raw/c4901df9a70f731acec3d0e852658d7bd0bb8e6f/src%2Fgc-stock.c",
      "contents_url": "https://api.github.com/repos/JuliaLang/julia/contents/src%2Fgc-stock.c?ref=c4901df9a70f731acec3d0e852658d7bd0bb8e6f",
      "patch": "@@ -1,48 +1,76 @@\n // This file is a part of Julia. License is MIT: https://julialang.org/license\n \n-#include \"gc-common.h\"\n-#include \"gc-stock.h\"\n #include \"gc-alloc-profiler.h\"\n+#include \"gc-common.h\"\n #include \"gc-heap-snapshot.h\"\n #include \"gc-page-profiler.h\"\n+#include \"gc-stock.h\"\n #include \"julia.h\"\n+#include \"julia_assert.h\"\n #include \"julia_atomics.h\"\n #include \"julia_gcext.h\"\n-#include \"julia_assert.h\"\n \n #ifdef __cplusplus\n extern \"C\" {\n #endif\n \n+// System-wide heap statistics\n+gc_heapstatus_t gc_heap_stats = {0};\n+\n+// Memory upper bound on 32-bit systems\n+const uint64_t max_mem_32bit_systems = 1536 * 1024 * 1024; // 1.5 GiB\n+// Julia's GC heuristics will try to keep the heap size below the `max_total_memory` soft limit,\n+// but they are allowed to exceed it, instead of aborting the process.\n+// This parameter can be changed via `jl_gc_set_max_memory()`.\n+#ifdef _P64\n+#define PETA_BYTE (1024ULL * 1024 * 1024 * 1024 * 1024)\n+static uint64_t max_total_memory = 2 * PETA_BYTE;\n+#else\n+static uint64_t max_total_memory = max_mem_32bit_systems;\n+#endif\n+\n+#ifdef _P64\n+static const size_t default_collect_interval = 5600 * 1024 * sizeof(void*); // ~45 MiB\n+#else\n+static const size_t default_collect_interval = 3200 * 1024 * sizeof(void*); // ~12 MiB\n+#endif\n+\n+// ID of first GC thread\n+int gc_first_tid;\n // Number of GC threads that may run parallel marking\n int jl_n_markthreads;\n-// Number of GC threads that may run concurrent sweeping (0 or 1)\n-int jl_n_sweepthreads;\n // Number of threads currently running the GC mark-loop\n _Atomic(int) gc_n_threads_marking;\n+// ID of mutator thread that triggered GC\n+_Atomic(int) gc_initiator_tid;\n+// Mutex/cond used to synchronize wakeup of GC threads on parallel marking\n+uv_mutex_t gc_threads_lock;\n+uv_cond_t gc_threads_cond;\n+// Mutex used to coordinate entry of GC threads in the mark loop\n+uv_mutex_t gc_queue_observer_lock;\n+\n // Number of threads sweeping\n _Atomic(int) gc_n_threads_sweeping_pools;\n-// Number of threads sweeping stacks\n-_Atomic(int) gc_n_threads_sweeping_stacks;\n // Temporary for the `ptls->gc_tls.page_metadata_allocd` used during parallel sweeping (padded to avoid false sharing)\n _Atomic(jl_gc_padded_page_stack_t *) gc_allocd_scratch;\n-// `tid` of mutator thread that triggered GC\n-_Atomic(int) gc_initiator_tid;\n+\n+// Number of GC threads that may run concurrent sweeping (0 or 1)\n+int jl_n_sweepthreads;\n+// To indicate whether concurrent sweeping should run\n+uv_sem_t gc_sweep_assists_needed;\n+\n+// Number of threads sweeping stacks\n+_Atomic(int) gc_n_threads_sweeping_stacks;\n // counter for sharing work when sweeping stacks\n _Atomic(int) gc_ptls_sweep_idx;\n // counter for round robin of giving back stack pages to the OS\n-_Atomic(int) gc_stack_free_idx = 0;\n-// `tid` of first GC thread\n-int gc_first_tid;\n-// Mutex/cond used to synchronize wakeup of GC threads on parallel marking\n-uv_mutex_t gc_threads_lock;\n-uv_cond_t gc_threads_cond;\n-// To indicate whether concurrent sweeping should run\n-uv_sem_t gc_sweep_assists_needed;\n-// Mutex used to coordinate entry of GC threads in the mark loop\n-uv_mutex_t gc_queue_observer_lock;\n+_Atomic(int) gc_stack_free_idx;\n+\n // Tag for sentinel nodes in bigval list\n uintptr_t gc_bigval_sentinel_tag;\n+// List of big objects in oldest generation (`GC_OLD_MARKED`).  Not per-thread.  Accessed only by master thread.\n+bigval_t *oldest_generation_of_bigvals = NULL;\n+\n // Table recording number of full GCs due to each reason\n JL_DLLEXPORT uint64_t jl_full_sweep_reasons[FULL_SWEEP_NUM_REASONS];\n \n@@ -82,11 +110,6 @@ static _Atomic(int) support_conservative_marking = 0;\n  * finalizers in unmanaged (GC safe) mode.\n  */\n \n-gc_heapstatus_t gc_heap_stats = {0};\n-\n-// List of big objects in oldest generation (`GC_OLD_MARKED`).  Not per-thread.  Accessed only by master thread.\n-bigval_t *oldest_generation_of_bigvals = NULL;\n-\n // explicitly scheduled objects for the sweepfunc callback\n static void gc_sweep_foreign_objs_in_list(arraylist_t *objs) JL_NOTSAFEPOINT\n {\n@@ -108,7 +131,7 @@ static void gc_sweep_foreign_objs_in_list(arraylist_t *objs) JL_NOTSAFEPOINT\n \n static void gc_sweep_foreign_objs(void) JL_NOTSAFEPOINT\n {\n-    assert(gc_n_threads);\n+    assert(gc_n_threads != 0);\n     for (int i = 0; i < gc_n_threads; i++) {\n         jl_ptls_t ptls2 = gc_all_tls_states[i];\n         if (ptls2 != NULL)\n@@ -119,23 +142,6 @@ static void gc_sweep_foreign_objs(void) JL_NOTSAFEPOINT\n // GC knobs and self-measurement variables\n static int64_t last_gc_total_bytes = 0;\n \n-// max_total_memory is a suggestion.  We try very hard to stay\n-// under this limit, but we will go above it rather than halting.\n-#ifdef _P64\n-typedef uint64_t memsize_t;\n-static const size_t default_collect_interval = 5600 * 1024 * sizeof(void*);\n-static size_t total_mem;\n-// We expose this to the user/ci as jl_gc_set_max_memory\n-static memsize_t max_total_memory = (memsize_t) 2 * 1024 * 1024 * 1024 * 1024 * 1024;\n-#else\n-typedef uint32_t memsize_t;\n-static const size_t default_collect_interval = 3200 * 1024 * sizeof(void*);\n-// Work really hard to stay within 2GB\n-// Alternative is to risk running out of address space\n-// on 32 bit architectures.\n-#define MAX32HEAP 1536 * 1024 * 1024\n-static memsize_t max_total_memory = (memsize_t) MAX32HEAP;\n-#endif\n // heuristic stuff for https://dl.acm.org/doi/10.1145/3563323\n // start with values that are in the target ranges to reduce transient hiccups at startup\n static uint64_t old_pause_time = 1e7; // 10 ms\n@@ -222,7 +228,7 @@ static void gc_sync_cache(jl_ptls_t ptls, jl_gc_mark_cache_t *gc_cache) JL_NOTSA\n // No other threads can be running marking at the same time\n static void gc_sync_all_caches(jl_ptls_t ptls) JL_NOTSAFEPOINT\n {\n-    assert(gc_n_threads);\n+    assert(gc_n_threads != 0);\n     for (int t_i = 0; t_i < gc_n_threads; t_i++) {\n         jl_ptls_t ptls2 = gc_all_tls_states[t_i];\n         if (ptls2 != NULL)\n@@ -366,7 +372,7 @@ JL_DLLEXPORT jl_weakref_t *jl_gc_new_weakref_th(jl_ptls_t ptls, jl_value_t *valu\n \n static void clear_weak_refs(void) JL_NOTSAFEPOINT\n {\n-    assert(gc_n_threads);\n+    assert(gc_n_threads != 0);\n     for (int i = 0; i < gc_n_threads; i++) {\n         jl_ptls_t ptls2 = gc_all_tls_states[i];\n         if (ptls2 != NULL) {\n@@ -383,24 +389,25 @@ static void clear_weak_refs(void) JL_NOTSAFEPOINT\n \n static void sweep_weak_refs(void) JL_NOTSAFEPOINT\n {\n-    assert(gc_n_threads);\n+    assert(gc_n_threads != 0);\n     for (int i = 0; i < gc_n_threads; i++) {\n         jl_ptls_t ptls2 = gc_all_tls_states[i];\n-        if (ptls2 != NULL) {\n-            size_t n = 0;\n-            size_t i = 0;\n-            size_t l = ptls2->gc_tls_common.heap.weak_refs.len;\n-            void **lst = ptls2->gc_tls_common.heap.weak_refs.items;\n-            // filter with preserving order\n-            for (i = 0; i < l; i++) {\n-                jl_weakref_t *wr = (jl_weakref_t*)lst[i];\n-                if (gc_marked(jl_astaggedvalue(wr)->bits.gc)) {\n-                    lst[n] = wr;\n-                    n++;\n-                }\n+        if (ptls2 == NULL) {\n+            continue;\n+        }\n+        size_t n = 0;\n+        size_t i = 0;\n+        size_t l = ptls2->gc_tls_common.heap.weak_refs.len;\n+        void **lst = ptls2->gc_tls_common.heap.weak_refs.items;\n+        // filter with preserving order\n+        for (i = 0; i < l; i++) {\n+            jl_weakref_t *wr = (jl_weakref_t*)lst[i];\n+            if (gc_marked(jl_astaggedvalue(wr)->bits.gc)) {\n+                lst[n] = wr;\n+                n++;\n             }\n-            ptls2->gc_tls_common.heap.weak_refs.len = n;\n         }\n+        ptls2->gc_tls_common.heap.weak_refs.len = n;\n     }\n }\n \n@@ -530,15 +537,16 @@ static void sweep_list_of_oldest_bigvals(bigval_t *young) JL_NOTSAFEPOINT\n static void sweep_big(jl_ptls_t ptls) JL_NOTSAFEPOINT\n {\n     gc_time_big_start();\n-    assert(gc_n_threads);\n+    assert(gc_n_threads != 0);\n     bigval_t *last_node_in_my_list = NULL;\n     for (int i = 0; i < gc_n_threads; i++) {\n         jl_ptls_t ptls2 = gc_all_tls_states[i];\n-        if (ptls2 != NULL) {\n-            bigval_t *last_node = sweep_list_of_young_bigvals(ptls2->gc_tls.heap.young_generation_of_bigvals);\n-            if (ptls == ptls2) {\n-                last_node_in_my_list = last_node;\n-            }\n+        if (ptls2 == NULL) {\n+            continue;\n+    }\n+        bigval_t *last_node = sweep_list_of_young_bigvals(ptls2->gc_tls.heap.young_generation_of_bigvals);\n+        if (ptls == ptls2) {\n+            last_node_in_my_list = last_node;\n         }\n     }\n     if (current_sweep_full) {\n@@ -572,20 +580,21 @@ static void combine_thread_gc_counts(jl_gc_num_t *dest, int update_heap) JL_NOTS\n     gc_all_tls_states = jl_atomic_load_relaxed(&jl_all_tls_states);\n     for (int i = 0; i < gc_n_threads; i++) {\n         jl_ptls_t ptls = gc_all_tls_states[i];\n-        if (ptls) {\n-            dest->allocd += (jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.allocd) + gc_num.interval);\n-            dest->malloc += jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.malloc);\n-            dest->realloc += jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.realloc);\n-            dest->poolalloc += jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.poolalloc);\n-            dest->bigalloc += jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.bigalloc);\n-            dest->freed += jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.free_acc);\n-            if (update_heap) {\n-                uint64_t alloc_acc = jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.alloc_acc);\n-                freed_in_runtime += jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.free_acc);\n-                jl_atomic_store_relaxed(&gc_heap_stats.heap_size, alloc_acc + jl_atomic_load_relaxed(&gc_heap_stats.heap_size));\n-                jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.alloc_acc, 0);\n-                jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.free_acc, 0);\n-            }\n+        if (ptls == NULL) {\n+            continue;\n+        }\n+        dest->allocd += (jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.allocd) + gc_num.interval);\n+        dest->malloc += jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.malloc);\n+        dest->realloc += jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.realloc);\n+        dest->poolalloc += jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.poolalloc);\n+        dest->bigalloc += jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.bigalloc);\n+        dest->freed += jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.free_acc);\n+        if (update_heap) {\n+            uint64_t alloc_acc = jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.alloc_acc);\n+            freed_in_runtime += jl_atomic_load_relaxed(&ptls->gc_tls_common.gc_num.free_acc);\n+            jl_atomic_store_relaxed(&gc_heap_stats.heap_size, alloc_acc + jl_atomic_load_relaxed(&gc_heap_stats.heap_size));\n+            jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.alloc_acc, 0);\n+            jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.free_acc, 0);\n         }\n     }\n }\n@@ -598,29 +607,26 @@ static void reset_thread_gc_counts(void) JL_NOTSAFEPOINT\n     gc_all_tls_states = jl_atomic_load_relaxed(&jl_all_tls_states);\n     for (int i = 0; i < gc_n_threads; i++) {\n         jl_ptls_t ptls = gc_all_tls_states[i];\n-        if (ptls != NULL) {\n-            // don't reset `pool_live_bytes` here\n-            jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.allocd, -(int64_t)gc_num.interval);\n-            jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.malloc, 0);\n-            jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.realloc, 0);\n-            jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.poolalloc, 0);\n-            jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.bigalloc, 0);\n-            jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.alloc_acc, 0);\n-            jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.free_acc, 0);\n+        if (ptls == NULL) {\n+            continue;\n         }\n+        // don't reset `pool_live_bytes` here\n+        jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.allocd, -(int64_t)gc_num.interval);\n+        jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.malloc, 0);\n+        jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.realloc, 0);\n+        jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.poolalloc, 0);\n+        jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.bigalloc, 0);\n+        jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.alloc_acc, 0);\n+        jl_atomic_store_relaxed(&ptls->gc_tls_common.gc_num.free_acc, 0);\n     }\n }\n \n-static int64_t inc_live_bytes(int64_t inc) JL_NOTSAFEPOINT\n-{\n-    jl_timing_counter_inc(JL_TIMING_COUNTER_HeapSize, inc);\n-    return live_bytes += inc;\n-}\n-\n void jl_gc_reset_alloc_count(void) JL_NOTSAFEPOINT\n {\n     combine_thread_gc_counts(&gc_num, 0);\n-    inc_live_bytes(gc_num.deferred_alloc + gc_num.allocd);\n+    int64_t alloc_increment = gc_num.deferred_alloc + gc_num.allocd;\n+    jl_timing_counter_inc(JL_TIMING_COUNTER_HeapSize, alloc_increment);\n+    live_bytes += alloc_increment;\n     gc_num.allocd = 0;\n     gc_num.deferred_alloc = 0;\n     reset_thread_gc_counts();\n@@ -646,28 +652,29 @@ static void jl_gc_free_memory(jl_genericmemory_t *m, int isaligned) JL_NOTSAFEPO\n static void sweep_malloced_memory(void) JL_NOTSAFEPOINT\n {\n     gc_time_mallocd_memory_start();\n-    assert(gc_n_threads);\n+    assert(gc_n_threads != 0);\n     for (int t_i = 0; t_i < gc_n_threads; t_i++) {\n         jl_ptls_t ptls2 = gc_all_tls_states[t_i];\n-        if (ptls2 != NULL) {\n-            size_t n = 0;\n-            size_t l = ptls2->gc_tls_common.heap.mallocarrays.len;\n-            void **lst = ptls2->gc_tls_common.heap.mallocarrays.items;\n-            // filter without preserving order\n-            while (n < l) {\n-                jl_genericmemory_t *m = (jl_genericmemory_t*)((uintptr_t)lst[n] & ~1);\n-                if (gc_marked(jl_astaggedvalue(m)->bits.gc)) {\n-                    n++;\n-                }\n-                else {\n-                    int isaligned = (uintptr_t)lst[n] & 1;\n-                    jl_gc_free_memory(m, isaligned);\n-                    l--;\n-                    lst[n] = lst[l];\n-                }\n+        if (ptls2 == NULL) {\n+            continue;\n+        }\n+        size_t n = 0;\n+        size_t l = ptls2->gc_tls_common.heap.mallocarrays.len;\n+        void **lst = ptls2->gc_tls_common.heap.mallocarrays.items;\n+        // filter without preserving order\n+        while (n < l) {\n+            jl_genericmemory_t *m = (jl_genericmemory_t*)((uintptr_t)lst[n] & ~1);\n+            if (gc_marked(jl_astaggedvalue(m)->bits.gc)) {\n+                n++;\n+            }\n+            else {\n+                int isaligned = (uintptr_t)lst[n] & 1;\n+                jl_gc_free_memory(m, isaligned);\n+                l--;\n+                lst[n] = lst[l];\n             }\n-            ptls2->gc_tls_common.heap.mallocarrays.len = l;\n         }\n+        ptls2->gc_tls_common.heap.mallocarrays.len = l;\n     }\n     gc_time_mallocd_memory_end();\n }\n@@ -1045,7 +1052,7 @@ void sweep_stack_pool_loop(void) JL_NOTSAFEPOINT\n         jl_ptls_t ptls2 = gc_all_tls_states[i];\n         if (ptls2 == NULL)\n             continue;\n-        assert(gc_n_threads);\n+        assert(gc_n_threads != 0);\n         // free half of stacks that remain unused since last sweep\n         if (i == jl_atomic_load_relaxed(&gc_stack_free_idx)) {\n             for (int p = 0; p < JL_N_STACK_POOLS; p++) {\n@@ -1120,7 +1127,7 @@ void sweep_stack_pool_loop(void) JL_NOTSAFEPOINT\n JL_DLLEXPORT void jl_gc_sweep_stack_pools_and_mtarraylist_buffers(jl_ptls_t ptls) JL_NOTSAFEPOINT\n {\n     // initialize ptls index for parallel sweeping of stack pools\n-    assert(gc_n_threads);\n+    assert(gc_n_threads != 0);\n     int stack_free_idx = jl_atomic_load_relaxed(&gc_stack_free_idx);\n     if (stack_free_idx + 1 == gc_n_threads)\n         jl_atomic_store_relaxed(&gc_stack_free_idx, 0);\n@@ -2010,59 +2017,6 @@ STATIC_INLINE void gc_mark_memory16(jl_ptls_t ptls, jl_value_t *ary16_parent, jl\n     }\n }\n \n-// Mark chunk of large array\n-STATIC_INLINE void gc_mark_chunk(jl_ptls_t ptls, jl_gc_markqueue_t *mq, jl_gc_chunk_t *c) JL_NOTSAFEPOINT\n-{\n-    switch (c->cid) {\n-        case GC_objary_chunk: {\n-            jl_value_t *obj_parent = c->parent;\n-            jl_value_t **obj_begin = c->begin;\n-            jl_value_t **obj_end = c->end;\n-            uint32_t step = c->step;\n-            uintptr_t nptr = c->nptr;\n-            gc_mark_objarray(ptls, obj_parent, obj_begin, obj_end,\n-                             step, nptr);\n-            break;\n-        }\n-        case GC_ary8_chunk: {\n-            jl_value_t *ary8_parent = c->parent;\n-            jl_value_t **ary8_begin = c->begin;\n-            jl_value_t **ary8_end = c->end;\n-            uint8_t *elem_begin = (uint8_t *)c->elem_begin;\n-            uint8_t *elem_end = (uint8_t *)c->elem_end;\n-            size_t elsize = c->step;\n-            uintptr_t nptr = c->nptr;\n-            gc_mark_memory8(ptls, ary8_parent, ary8_begin, ary8_end, elem_begin, elem_end,\n-                           elsize, nptr);\n-            break;\n-        }\n-        case GC_ary16_chunk: {\n-            jl_value_t *ary16_parent = c->parent;\n-            jl_value_t **ary16_begin = c->begin;\n-            jl_value_t **ary16_end = c->end;\n-            uint16_t *elem_begin = (uint16_t *)c->elem_begin;\n-            uint16_t *elem_end = (uint16_t *)c->elem_end;\n-            size_t elsize = c->step;\n-            uintptr_t nptr = c->nptr;\n-            gc_mark_memory16(ptls, ary16_parent, ary16_begin, ary16_end, elem_begin, elem_end,\n-                            elsize, nptr);\n-            break;\n-        }\n-        case GC_finlist_chunk: {\n-            jl_value_t *fl_parent = c->parent;\n-            jl_value_t **fl_begin = c->begin;\n-            jl_value_t **fl_end = c->end;\n-            gc_mark_finlist_(mq, fl_parent, fl_begin, fl_end);\n-            break;\n-        }\n-        default: {\n-            // `empty-chunk` should be checked by caller\n-            jl_safe_printf(\"GC internal error: chunk mismatch\\n\");\n-            abort();\n-        }\n-    }\n-}\n-\n // Mark gc frame\n STATIC_INLINE void gc_mark_stack(jl_ptls_t ptls, jl_gcframe_t *s, uint32_t nroots, uintptr_t offset,\n                    uintptr_t lb, uintptr_t ub) JL_NOTSAFEPOINT\n@@ -2177,7 +2131,7 @@ STATIC_INLINE void gc_mark_module_binding(jl_ptls_t ptls, jl_module_t *mb_parent\n     }\n }\n \n-void gc_mark_finlist_(jl_gc_markqueue_t *mq, jl_value_t *fl_parent, jl_value_t **fl_begin, jl_value_t **fl_end)\n+void gc_mark_finlist_(jl_gc_markqueue_t *mq, jl_value_t *fl_parent, jl_value_t **fl_begin, jl_value_t **fl_end) JL_NOTSAFEPOINT\n {\n     jl_value_t *new_obj;\n     // Decide whether need to chunk finlist\n@@ -2222,6 +2176,59 @@ void gc_mark_finlist(jl_gc_markqueue_t *mq, arraylist_t *list, size_t start)\n     gc_mark_finlist_(mq, NULL, fl_begin, fl_end);\n }\n \n+// Mark chunk of large array\n+STATIC_INLINE void gc_mark_chunk(jl_ptls_t ptls, jl_gc_markqueue_t *mq, jl_gc_chunk_t *c) JL_NOTSAFEPOINT\n+{\n+    switch (c->cid) {\n+        case GC_objary_chunk: {\n+            jl_value_t *obj_parent = c->parent;\n+            jl_value_t **obj_begin = c->begin;\n+            jl_value_t **obj_end = c->end;\n+            uint32_t step = c->step;\n+            uintptr_t nptr = c->nptr;\n+            gc_mark_objarray(ptls, obj_parent, obj_begin, obj_end,\n+                             step, nptr);\n+            break;\n+        }\n+        case GC_ary8_chunk: {\n+            jl_value_t *ary8_parent = c->parent;\n+            jl_value_t **ary8_begin = c->begin;\n+            jl_value_t **ary8_end = c->end;\n+            uint8_t *elem_begin = (uint8_t *)c->elem_begin;\n+            uint8_t *elem_end = (uint8_t *)c->elem_end;\n+            size_t elsize = c->step;\n+            uintptr_t nptr = c->nptr;\n+            gc_mark_memory8(ptls, ary8_parent, ary8_begin, ary8_end, elem_begin, elem_end,\n+                           elsize, nptr);\n+            break;\n+        }\n+        case GC_ary16_chunk: {\n+            jl_value_t *ary16_parent = c->parent;\n+            jl_value_t **ary16_begin = c->begin;\n+            jl_value_t **ary16_end = c->end;\n+            uint16_t *elem_begin = (uint16_t *)c->elem_begin;\n+            uint16_t *elem_end = (uint16_t *)c->elem_end;\n+            size_t elsize = c->step;\n+            uintptr_t nptr = c->nptr;\n+            gc_mark_memory16(ptls, ary16_parent, ary16_begin, ary16_end, elem_begin, elem_end,\n+                            elsize, nptr);\n+            break;\n+        }\n+        case GC_finlist_chunk: {\n+            jl_value_t *fl_parent = c->parent;\n+            jl_value_t **fl_begin = c->begin;\n+            jl_value_t **fl_end = c->end;\n+            gc_mark_finlist_(mq, fl_parent, fl_begin, fl_end);\n+            break;\n+        }\n+        default: {\n+            // `empty-chunk` should be checked by caller\n+            jl_safe_printf(\"GC internal error: unknown chunk type\\n\");\n+            abort();\n+        }\n+    }\n+}\n+\n JL_DLLEXPORT int jl_gc_mark_queue_obj(jl_ptls_t ptls, jl_value_t *obj)\n {\n     int may_claim = gc_try_setmark_tag(jl_astaggedvalue(obj), GC_MARKED);\n@@ -2999,6 +3006,27 @@ static uint64_t overallocation(uint64_t old_val, uint64_t val, uint64_t max_val)\n \n size_t jl_maxrss(void);\n \n+void _report_gc_finished(uint64_t pause, uint64_t freed, int full, int recollect, int64_t live_bytes) JL_NOTSAFEPOINT {\n+    if (!gc_logging_enabled) {\n+        return;\n+    }\n+    jl_safe_printf(\"\\nGC: pause %.2fms. collected %fMB. %s %s\\n\",\n+        pause/1e6, freed/(double)(1<<20),\n+        full ? \"full\" : \"incr\",\n+        recollect ? \"recollect\" : \"\"\n+    );\n+\n+    jl_safe_printf(\"Heap stats: bytes_mapped %.2f MB, bytes_resident %.2f MB,\\nheap_size %.2f MB, heap_target %.2f MB, Fragmentation %.3f\\n\",\n+        jl_atomic_load_relaxed(&gc_heap_stats.bytes_mapped)/(double)(1<<20),\n+        jl_atomic_load_relaxed(&gc_heap_stats.bytes_resident)/(double)(1<<20),\n+        // live_bytes/(double)(1<<20), live byes tracking is not accurate.\n+        jl_atomic_load_relaxed(&gc_heap_stats.heap_size)/(double)(1<<20),\n+        jl_atomic_load_relaxed(&gc_heap_stats.heap_target)/(double)(1<<20),\n+        (double)live_bytes/(double)jl_atomic_load_relaxed(&gc_heap_stats.heap_size)\n+    );\n+    // Should fragmentation use bytes_resident instead of heap_size?\n+}\n+\n // Only one thread should be running in this function\n static int _jl_gc_collect(jl_ptls_t ptls, jl_gc_collection_t collection) JL_NOTSAFEPOINT\n {\n@@ -3018,7 +3046,7 @@ static int _jl_gc_collect(jl_ptls_t ptls, jl_gc_collection_t collection) JL_NOTS\n     JL_PROBE_GC_MARK_BEGIN();\n     {\n         JL_TIMING(GC, GC_Mark);\n-        assert(gc_n_threads);\n+        assert(gc_n_threads != 0);\n         int single_threaded_mark = (jl_n_markthreads == 0 || gc_heap_snapshot_enabled);\n         for (int t_i = 0; t_i < gc_n_threads; t_i++) {\n             jl_ptls_t ptls2 = gc_all_tls_states[t_i];\n@@ -3063,7 +3091,7 @@ static int _jl_gc_collect(jl_ptls_t ptls, jl_gc_collection_t collection) JL_NOTS\n         // mark the object moved to the marked list from the\n         // `finalizer_list` by `sweep_finalizer_list`\n         size_t orig_marked_len = finalizer_list_marked.len;\n-        assert(gc_n_threads);\n+        assert(gc_n_threads != 0);\n         for (int i = 0; i < gc_n_threads; i++) {\n             jl_ptls_t ptls2 = gc_all_tls_states[i];\n             if (ptls2 != NULL)\n@@ -3073,7 +3101,7 @@ static int _jl_gc_collect(jl_ptls_t ptls, jl_gc_collection_t collection) JL_NOTS\n             sweep_finalizer_list(&finalizer_list_marked);\n             orig_marked_len = 0;\n         }\n-        assert(gc_n_threads);\n+        assert(gc_n_threads != 0);\n         for (int i = 0; i < gc_n_threads; i++) {\n             jl_ptls_t ptls2 = gc_all_tls_states[i];\n             if (ptls2 != NULL)\n@@ -3119,7 +3147,7 @@ static int _jl_gc_collect(jl_ptls_t ptls, jl_gc_collection_t collection) JL_NOTS\n     int remset_nptr = 0;\n     int sweep_full = next_sweep_full;\n     int recollect = 0;\n-    assert(gc_n_threads);\n+    assert(gc_n_threads != 0);\n     for (int i = 0; i < gc_n_threads; i++) {\n         jl_ptls_t ptls2 = gc_all_tls_states[i];\n         if (ptls2 != NULL)\n@@ -3133,12 +3161,12 @@ static int _jl_gc_collect(jl_ptls_t ptls, jl_gc_collection_t collection) JL_NOTS\n     // we either free some space or get an OOM error.\n     if (jl_options.gc_sweep_always_full) {\n         sweep_full = 1;\n-        gc_count_full_sweep_reason(FULL_SWEEP_REASON_SWEEP_ALWAYS_FULL);\n+        gc_record_full_sweep_reason(FULL_SWEEP_REASON_SWEEP_ALWAYS_FULL);\n     }\n     if (collection == JL_GC_FULL && !prev_sweep_full) {\n         sweep_full = 1;\n         recollect = 1;\n-        gc_count_full_sweep_reason(FULL_SWEEP_REASON_FORCED_FULL_SWEEP);\n+        gc_record_full_sweep_reason(FULL_SWEEP_REASON_FORCED_FULL_SWEEP);\n     }\n     if (sweep_full) {\n         // these are the difference between the number of gc-perm bytes scanned\n@@ -3302,11 +3330,11 @@ static int _jl_gc_collect(jl_ptls_t ptls, jl_gc_collection_t collection) JL_NOTS\n     double old_ratio = (double)promoted_bytes/(double)heap_size;\n     if (heap_size > user_max) {\n         next_sweep_full = 1;\n-        gc_count_full_sweep_reason(FULL_SWEEP_REASON_USER_MAX_EXCEEDED);\n+        gc_record_full_sweep_reason(FULL_SWEEP_REASON_USER_MAX_EXCEEDED);\n     }\n     else if (old_ratio > 0.15) {\n         next_sweep_full = 1;\n-        gc_count_full_sweep_reason(FULL_SWEEP_REASON_LARGE_PROMOTION_RATE);\n+        gc_record_full_sweep_reason(FULL_SWEEP_REASON_LARGE_PROMOTION_RATE);\n     }\n     else {\n         next_sweep_full = 0;\n@@ -3316,7 +3344,7 @@ static int _jl_gc_collect(jl_ptls_t ptls, jl_gc_collection_t collection) JL_NOTS\n     // sweeping is over\n     // 6. if it is a quick sweep, put back the remembered objects in queued state\n     // so that we don't trigger the barrier again on them.\n-    assert(gc_n_threads);\n+    assert(gc_n_threads != 0);\n     for (int t_i = 0; t_i < gc_n_threads; t_i++) {\n         jl_ptls_t ptls2 = gc_all_tls_states[t_i];\n         if (ptls2 == NULL)\n@@ -3502,7 +3530,7 @@ JL_DLLEXPORT void jl_gc_collect(jl_gc_collection_t collection)\n \n void gc_mark_queue_all_roots(jl_ptls_t ptls, jl_gc_markqueue_t *mq)\n {\n-    assert(gc_n_threads);\n+    assert(gc_n_threads != 0);\n     for (size_t i = 0; i < gc_n_threads; i++) {\n         jl_ptls_t ptls2 = gc_all_tls_states[i];\n         if (ptls2 != NULL)\n@@ -3570,6 +3598,7 @@ void jl_free_thread_gc_state(jl_ptls_t ptls)\n     arraylist_free(&mq->reclaim_set);\n }\n \n+extern uv_barrier_t thread_init_done;\n void jl_start_gc_threads(void)\n {\n     int nthreads = jl_atomic_load_relaxed(&jl_n_threads);\n@@ -3603,6 +3632,8 @@ STATIC_INLINE int may_sweep_stack(jl_ptls_t ptls) JL_NOTSAFEPOINT\n {\n     return (jl_atomic_load(&ptls->gc_tls.gc_stack_sweep_requested) > 0);\n }\n+\n+extern _Atomic(int) n_threads_running;\n // parallel gc thread function\n void jl_parallel_gc_threadfun(void *arg)\n {\n@@ -3714,7 +3745,7 @@ void jl_gc_init(void)\n             hint = parse_heap_size_option(cp, \"JULIA_HEAP_SIZE_HINT=\\\"<size>[<unit>]\\\"\", 1);\n     }\n #ifdef _P64\n-    total_mem = uv_get_total_memory();\n+    size_t total_mem = uv_get_total_memory();\n     if (hint == 0) {\n         uint64_t constrained_mem = uv_get_constrained_memory();\n         if (constrained_mem > 0 && constrained_mem < total_mem)\n@@ -3731,7 +3762,7 @@ void jl_gc_init(void)\n JL_DLLEXPORT void jl_gc_set_max_memory(uint64_t max_mem)\n {\n #ifdef _P32\n-    max_mem = max_mem < MAX32HEAP ? max_mem : MAX32HEAP;\n+    max_mem = max_mem < max_mem_32bit_systems ? max_mem : max_mem_32bit_systems;\n #endif\n     max_total_memory = max_mem;\n }\n@@ -4084,7 +4115,8 @@ void jl_gc_notify_image_load(const char* img_data, size_t len)\n     // Do nothing\n }\n \n-JL_DLLEXPORT const char* jl_gc_active_impl(void) {\n+JL_DLLEXPORT const char* jl_gc_active_impl(void)\n+{\n     return \"Built with stock GC\";\n }\n "
    },
    {
      "sha": "4067093b39de7f3a8fca8bc65b6058711874026e",
      "filename": "src/gc-stock.h",
      "status": "modified",
      "additions": 186,
      "deletions": 123,
      "changes": 309,
      "blob_url": "https://github.com/JuliaLang/julia/blob/c4901df9a70f731acec3d0e852658d7bd0bb8e6f/src%2Fgc-stock.h",
      "raw_url": "https://github.com/JuliaLang/julia/raw/c4901df9a70f731acec3d0e852658d7bd0bb8e6f/src%2Fgc-stock.h",
      "contents_url": "https://api.github.com/repos/JuliaLang/julia/contents/src%2Fgc-stock.h?ref=c4901df9a70f731acec3d0e852658d7bd0bb8e6f",
      "patch": "@@ -1,25 +1,41 @@\n // This file is a part of Julia. License is MIT: https://julialang.org/license\n \n /*\n-  allocation and garbage collection\n-  . non-moving, precise mark and sweep collector\n-  . pool-allocates small objects, keeps big objects on a simple list\n-*/\n+ * Julia implements a garbage collector (GC) to automate dynamic memory management.\n+ * Key characteristics of Julia's stock GC:\n+ *\n+ * - Mark-sweep: The object graph is traced starting from a root set\n+ *   (e.g., global variables and local variables on the stack) to determine live objects.\n+ *\n+ * - Non-moving: Objects are not relocated to a different memory address.\n+ *\n+ * - Parallel: Multiple threads can be used during the marking and sweeping phases.\n+ *\n+ * - Partially concurrent: The runtime can scavenge pool-allocated memory blocks\n+ *   (e.g., via madvise on Linux) concurrently with Julia user code.\n+ *\n+ * - Generational: Objects are partitioned into generations based on how many collection\n+ *   cycles they have survived. Younger generations are collected more often.\n+ *\n+ * - Mostly precise: Julia optionally supports conservative stack scanning for users\n+ *   interoperating with foreign languages like C.\n+ */\n+\n #ifndef JL_GC_H\n #define JL_GC_H\n \n+#include <inttypes.h>\n #include <stddef.h>\n #include <stdint.h>\n #include <stdlib.h>\n #include <string.h>\n #include <strings.h>\n-#include <inttypes.h>\n+#include \"gc-common.h\"\n #include \"julia.h\"\n-#include \"julia_threads.h\"\n-#include \"julia_internal.h\"\n #include \"julia_assert.h\"\n+#include \"julia_internal.h\"\n+#include \"julia_threads.h\"\n #include \"threading.h\"\n-#include \"gc-common.h\"\n \n #ifdef __cplusplus\n extern \"C\" {\n@@ -51,19 +67,19 @@ typedef struct {\n } jl_gc_debug_env_t;\n \n // Array chunks (work items representing suffixes of\n-// large arrays of pointers left to be marked)\n+// large arrays that have not been scanned yet)\n \n typedef enum {\n-    GC_empty_chunk = 0, // for sentinel representing no items left in chunk queue\n+    GC_empty_chunk = 0, // sentine value representing no chunk\n     GC_objary_chunk,    // for chunk of object array\n     GC_ary8_chunk,      // for chunk of array with 8 bit field descriptors\n     GC_ary16_chunk,     // for chunk of array with 16 bit field descriptors\n     GC_finlist_chunk,   // for chunk of finalizer list\n } gc_chunk_id_t;\n \n typedef struct _jl_gc_chunk_t {\n-    gc_chunk_id_t cid;\n-    struct _jl_value_t *parent; // array owner\n+    gc_chunk_id_t cid;          // chunk type identifier\n+    struct _jl_value_t *parent; // array parent\n     struct _jl_value_t **begin; // pointer to first element that needs scanning\n     struct _jl_value_t **end;   // pointer to last element that needs scanning\n     void *elem_begin;           // used to scan pointers within objects when marking `ary8` or `ary16`\n@@ -80,60 +96,67 @@ typedef struct _jl_gc_chunk_t {\n \n #define GC_REMSET_PTR_TAG (0x1)             // lowest bit of `jl_value_t *` is tagged if it's in the remset\n \n-// layout for big (>2k) objects\n-\n-extern uintptr_t gc_bigval_sentinel_tag;\n-\n-// pool page metadata\n+// Metadata structure that is paired with each pool-allocated page\n typedef struct _jl_gc_pagemeta_t {\n-    // next metadata structure in per-thread list\n-    // or in one of the `jl_gc_page_stack_t`\n+    // Pointer to the next metadata structure in the linked list\n     struct _jl_gc_pagemeta_t *next;\n-    // index of pool that owns this page\n+    // Index of the size class, in the pool allocator, that this metadata structure belongs to\n     uint8_t pool_n;\n     // Whether any cell in the page is marked\n     // This bit is set before sweeping iff there are live cells in the page.\n     // Note that before marking or after sweeping there can be live\n-    // (and young) cells in the page for `!has_marked`.\n+    // (and young) cells in the page for `!has_marked`\n     uint8_t has_marked;\n     // Whether any cell was live and young **before sweeping**.\n     // For a normal sweep (quick sweep that is NOT preceded by a\n     // full sweep) this bit is set iff there are young or newly dead\n-    // objects in the page and the page needs to be swept.\n+    // objects in the page and the page needs to be swept\n     //\n-    // For a full sweep, this bit should be ignored.\n+    // For a full sweep, this bit should be ignored\n     //\n     // For a quick sweep preceded by a full sweep. If this bit is set,\n     // the page needs to be swept. If this bit is not set, there could\n     // still be old dead objects in the page and `nold` and `prev_nold`\n-    // should be used to determine if the page needs to be swept.\n+    // should be used to determine if the page needs to be swept\n     uint8_t has_young;\n-    // number of old objects in this page\n+    // Number of old objects in the page\n     uint16_t nold;\n-    // number of old objects in this page during the previous full sweep\n+    // Number of old objects in the page at the end of the previous full sweep\n     uint16_t prev_nold;\n-    // number of free objects in this page.\n-    // invalid if pool that owns this page is allocating objects from this page.\n+    // Number of free objects in this page\n+    // Invalid if pool that owns this page is allocating objects from this page\n     uint16_t nfree;\n-    uint16_t osize;           // size of each object in this page\n-    uint16_t fl_begin_offset; // offset of first free object in this page\n-    uint16_t fl_end_offset;   // offset of last free object in this page\n-    uint16_t thread_n;        // thread id of the heap that owns this page\n-    char *data;\n+    uint16_t osize;           // Size of each object in this page\n+    uint16_t fl_begin_offset; // Offset of first free object in this page\n+    uint16_t fl_end_offset;   // Offset of last free object in this page\n+    uint16_t thread_n;        // Thread id of the heap that owns this page\n+    char *data;               // Pointer to the start of the regions where objects are allocated\n } jl_gc_pagemeta_t;\n \n extern jl_gc_page_stack_t global_page_pool_lazily_freed;\n extern jl_gc_page_stack_t global_page_pool_clean;\n extern jl_gc_page_stack_t global_page_pool_freed;\n \n-// Lock-free stack implementation taken\n-// from Herlihy's \"The Art of Multiprocessor Programming\"\n-// XXX: this is not a general-purpose lock-free stack. We can\n-// get away with just using a CAS and not implementing some ABA\n-// prevention mechanism since once a node is popped from the\n-// `jl_gc_page_stack_t`, it may only be pushed back to them\n-// in the sweeping phase, which also doesn't push a node into the\n-// same stack after it's popped\n+/*\n+ * Simple lock-free stack implementation for `jl_gc_page_stack_t`.\n+ *\n+ * NOTE: This is not a general-purpose lock-free stack. It does not implement\n+ * any ABA-prevention mechanism. For our specific use case, this is acceptable,\n+ * because we avoid the pathological concurrent push/pop sequences on the same\n+ * list node that could trigger the ABA problem.\n+ *\n+ * Safety invariants for this simple lock-free stack:\n+ *\n+ * 1. If a node is popped from the stack by a mutator thread, it will never\n+ *    be pushed back onto the same stack within the same GC epoch\n+ *    (i.e., the time window between two consecutive GCs).\n+ *\n+ * 2. If a node is popped by a GC thread, it will never be pushed back onto\n+ *    the same stack.\n+ *\n+ * These invariants ensure safe usage of this simplified lock-free stack\n+ * without requiring ABA prevention.\n+ */\n \n STATIC_INLINE void push_lf_back_nosync(jl_gc_page_stack_t *pool, jl_gc_pagemeta_t *elt) JL_NOTSAFEPOINT\n {\n@@ -142,16 +165,6 @@ STATIC_INLINE void push_lf_back_nosync(jl_gc_page_stack_t *pool, jl_gc_pagemeta_\n     jl_atomic_store_relaxed(&pool->bottom, elt);\n }\n \n-STATIC_INLINE jl_gc_pagemeta_t *pop_lf_back_nosync(jl_gc_page_stack_t *pool) JL_NOTSAFEPOINT\n-{\n-    jl_gc_pagemeta_t *old_back = jl_atomic_load_relaxed(&pool->bottom);\n-    if (old_back == NULL) {\n-        return NULL;\n-    }\n-    jl_atomic_store_relaxed(&pool->bottom, old_back->next);\n-    return old_back;\n-}\n-\n STATIC_INLINE void push_lf_back(jl_gc_page_stack_t *pool, jl_gc_pagemeta_t *elt) JL_NOTSAFEPOINT\n {\n     while (1) {\n@@ -164,11 +177,9 @@ STATIC_INLINE void push_lf_back(jl_gc_page_stack_t *pool, jl_gc_pagemeta_t *elt)\n     }\n }\n \n-#define MAX_POP_ATTEMPTS (1 << 10)\n-\n STATIC_INLINE jl_gc_pagemeta_t *try_pop_lf_back(jl_gc_page_stack_t *pool) JL_NOTSAFEPOINT\n {\n-    for (int i = 0; i < MAX_POP_ATTEMPTS; i++) {\n+    for (int i = 0; i < (1 << 10); i++) {\n         jl_gc_pagemeta_t *old_back = jl_atomic_load_relaxed(&pool->bottom);\n         if (old_back == NULL) {\n             return NULL;\n@@ -181,6 +192,17 @@ STATIC_INLINE jl_gc_pagemeta_t *try_pop_lf_back(jl_gc_page_stack_t *pool) JL_NOT\n     return NULL;\n }\n \n+STATIC_INLINE jl_gc_pagemeta_t *pop_lf_back_nosync(jl_gc_page_stack_t *pool) JL_NOTSAFEPOINT\n+{\n+    jl_gc_pagemeta_t *old_back = jl_atomic_load_relaxed(&pool->bottom);\n+    if (old_back == NULL) {\n+        return NULL;\n+    }\n+    jl_atomic_store_relaxed(&pool->bottom, old_back->next);\n+    return old_back;\n+}\n+\n+\n STATIC_INLINE jl_gc_pagemeta_t *pop_lf_back(jl_gc_page_stack_t *pool) JL_NOTSAFEPOINT\n {\n     while (1) {\n@@ -196,7 +218,11 @@ STATIC_INLINE jl_gc_pagemeta_t *pop_lf_back(jl_gc_page_stack_t *pool) JL_NOTSAFE\n }\n typedef struct {\n     jl_gc_page_stack_t stack;\n-    // pad to 128 bytes to avoid false-sharing\n+    /*\n+    * Pad to 128 bytes to avoid false sharing.\n+    * 128 bytes is large enough to ensure that two consecutively allocated\n+    * `jl_gc_padded_page_stack_t` instances will not share the same cache line.\n+    */\n #ifdef _P64\n     void *_pad[15];\n #else\n@@ -210,6 +236,42 @@ typedef struct {\n     _Atomic(size_t) n_pages_allocd;\n } gc_fragmentation_stat_t;\n \n+typedef struct {\n+    _Atomic(size_t) bytes_mapped;\n+    _Atomic(size_t) bytes_resident;\n+    _Atomic(size_t) heap_size;\n+    _Atomic(size_t) heap_target;\n+} gc_heapstatus_t;\n+\n+extern gc_heapstatus_t gc_heap_stats;\n+\n+/*\n+ * GC Multi-Level Page Table Structures\n+ *\n+ * Julia uses a hierarchical page table to track the allocation state of\n+ * pool-allocated memory pages. This design enables sparse memory representation\n+ * and fast lookup of page states.\n+ *\n+ * - Level 0: pagetable0_t\n+ *   - Lowest level of the page table.\n+ *   - Each entry in `meta` represents the state of a single GC page\n+ *     (GC_PAGE_UNMAPPED, GC_PAGE_ALLOCATED, etc.).\n+ *   - Size is determined by REGION0_PG_COUNT, which varies by page size and\n+ *     architecture.\n+ *\n+ * - Level 1: pagetable1_t\n+ *   - Middle level of the page table.\n+ *   - `meta0` points to Level 0 tables, each covering a contiguous region of pages.\n+ *   - Supports sparse allocation: entries can be NULL if no pages in that region\n+ *     are used.\n+ *\n+ * - Level 2 / Root: pagetable_t\n+ *   - Top-level root of the page table.\n+ *   - `meta1` points to Level 1 tables.\n+ *   - Provides the first lookup level for any heap pointer and supports large\n+ *     address spaces by subdividing memory into regions.\n+ */\n+\n #ifdef GC_SMALL_PAGE\n #ifdef _P64\n #define REGION0_PG_COUNT (1 << 16)\n@@ -244,7 +306,11 @@ typedef struct {\n #endif\n #endif\n \n-// define the representation of the levels of the page-table (0 to 2)\n+#define GC_PAGE_UNMAPPED        0\n+#define GC_PAGE_ALLOCATED       1\n+#define GC_PAGE_LAZILY_FREED    2\n+#define GC_PAGE_FREED           3\n+\n typedef struct {\n     uint8_t meta[REGION0_PG_COUNT];\n } pagetable0_t;\n@@ -257,18 +323,6 @@ typedef struct {\n     pagetable1_t *meta1[REGION2_PG_COUNT];\n } pagetable_t;\n \n-typedef struct {\n-    _Atomic(size_t) bytes_mapped;\n-    _Atomic(size_t) bytes_resident;\n-    _Atomic(size_t) heap_size;\n-    _Atomic(size_t) heap_target;\n-} gc_heapstatus_t;\n-\n-#define GC_PAGE_UNMAPPED        0\n-#define GC_PAGE_ALLOCATED       1\n-#define GC_PAGE_LAZILY_FREED    2\n-#define GC_PAGE_FREED           3\n-\n extern pagetable_t alloc_map;\n \n STATIC_INLINE uint8_t gc_alloc_map_is_set(char *_data) JL_NOTSAFEPOINT\n@@ -319,12 +373,47 @@ STATIC_INLINE void gc_alloc_map_maybe_create(char *_data) JL_NOTSAFEPOINT\n     }\n }\n \n-// Page layout:\n-//  Metadata pointer: sizeof(jl_gc_pagemeta_t*)\n-//  Padding: GC_PAGE_OFFSET - sizeof(jl_gc_pagemeta_t*)\n-//  Blocks: osize * n\n-//    Tag: sizeof(jl_taggedvalue_t)\n-//    Data: <= osize - sizeof(jl_taggedvalue_t)\n+/*\n+ * Page Layout\n+ *\n+ * Each pool-allocated page is divided into three main sections:\n+ *\n+ * - Metadata Pointer\n+ *   - Size: sizeof(jl_gc_pagemeta_t*)\n+ *   - Points to the page metadata structure.\n+ *\n+ * - Padding\n+ *   - Size: GC_PAGE_OFFSET - sizeof(jl_gc_pagemeta_t*)\n+ *   - Ensures proper alignment of the blocks.\n+ *\n+ * - Blocks\n+ *   - Size per block: osize\n+ *   - Each block consists of:\n+ *     - Tag: sizeof(jl_taggedvalue_t)\n+ *     - Data: up to (osize - sizeof(jl_taggedvalue_t))\n+ *\n+ * Example layout:\n+ *\n+ *   +----------------------+ <- page start\n+ *   | Metadata Pointer     |  sizeof(jl_gc_pagemeta_t*)\n+ *   +----------------------+\n+ *   | Padding              |  GC_PAGE_OFFSET - sizeof(jl_gc_pagemeta_t*)\n+ *   +----------------------+ <- GC_PAGE_OFFSET\n+ *   | Block 0              |  osize\n+ *   |   +----------------+|\n+ *   |   | Tag            || sizeof(jl_taggedvalue_t)\n+ *   |   +----------------+|\n+ *   |   | Data           || <= osize - sizeof(jl_taggedvalue_t)\n+ *   |   +----------------+|\n+ *   | Block 1              |  osize\n+ *   |   +----------------+|\n+ *   |   | Tag            || sizeof(jl_taggedvalue_t)\n+ *   |   +----------------+|\n+ *   |   | Data           || <= osize - sizeof(jl_taggedvalue_t)\n+ *   |   +----------------+|\n+ *   | ...                  |\n+ *   +----------------------+ <- page end\n+ */\n \n STATIC_INLINE char *gc_page_data(void *x) JL_NOTSAFEPOINT\n {\n@@ -364,10 +453,17 @@ STATIC_INLINE jl_gc_pagemeta_t *pop_page_metadata_back(jl_gc_pagemeta_t **ppg) J\n     return v;\n }\n \n-extern bigval_t *oldest_generation_of_bigvals;\n-extern int64_t buffered_pages;\n+STATIC_INLINE jl_taggedvalue_t *page_pfl_beg(jl_gc_pagemeta_t *p) JL_NOTSAFEPOINT\n+{\n+    return (jl_taggedvalue_t*)(p->data + p->fl_begin_offset);\n+}\n+\n+STATIC_INLINE jl_taggedvalue_t *page_pfl_end(jl_gc_pagemeta_t *p) JL_NOTSAFEPOINT\n+{\n+    return (jl_taggedvalue_t*)(p->data + p->fl_end_offset);\n+}\n+\n extern int gc_first_tid;\n-extern gc_heapstatus_t gc_heap_stats;\n \n STATIC_INLINE int gc_first_parallel_collector_thread_id(void) JL_NOTSAFEPOINT\n {\n@@ -427,21 +523,14 @@ STATIC_INLINE void gc_check_ptls_of_parallel_collector_thread(jl_ptls_t ptls) JL\n     assert(jl_atomic_load_relaxed(&ptls->gc_state) == JL_GC_PARALLEL_COLLECTOR_THREAD);\n }\n \n+extern uintptr_t gc_bigval_sentinel_tag;\n+extern bigval_t *oldest_generation_of_bigvals;\n+\n STATIC_INLINE bigval_t *bigval_header(jl_taggedvalue_t *o) JL_NOTSAFEPOINT\n {\n     return container_of(o, bigval_t, header);\n }\n \n-STATIC_INLINE jl_taggedvalue_t *page_pfl_beg(jl_gc_pagemeta_t *p) JL_NOTSAFEPOINT\n-{\n-    return (jl_taggedvalue_t*)(p->data + p->fl_begin_offset);\n-}\n-\n-STATIC_INLINE jl_taggedvalue_t *page_pfl_end(jl_gc_pagemeta_t *p) JL_NOTSAFEPOINT\n-{\n-    return (jl_taggedvalue_t*)(p->data + p->fl_end_offset);\n-}\n-\n FORCE_INLINE void gc_big_object_unlink(const bigval_t *node) JL_NOTSAFEPOINT\n {\n     assert(node != oldest_generation_of_bigvals);\n@@ -476,39 +565,27 @@ FORCE_INLINE void gc_big_object_link(bigval_t *sentinel_node, bigval_t *node) JL\n #define FULL_SWEEP_NUM_REASONS (4)\n \n extern JL_DLLEXPORT uint64_t jl_full_sweep_reasons[FULL_SWEEP_NUM_REASONS];\n-STATIC_INLINE void gc_count_full_sweep_reason(int reason) JL_NOTSAFEPOINT\n+STATIC_INLINE void gc_record_full_sweep_reason(int reason) JL_NOTSAFEPOINT\n {\n     assert(reason >= 0 && reason < FULL_SWEEP_NUM_REASONS);\n     jl_full_sweep_reasons[reason]++;\n }\n \n-extern uv_mutex_t gc_perm_lock;\n-extern uv_mutex_t gc_threads_lock;\n-extern uv_cond_t gc_threads_cond;\n-extern uv_sem_t gc_sweep_assists_needed;\n-extern _Atomic(int) gc_n_threads_marking;\n-extern _Atomic(int) gc_n_threads_sweeping_pools;\n-extern _Atomic(int) n_threads_running;\n-extern uv_barrier_t thread_init_done;\n-void gc_mark_queue_all_roots(jl_ptls_t ptls, jl_gc_markqueue_t *mq);\n-void gc_mark_finlist_(jl_gc_markqueue_t *mq, jl_value_t *fl_parent, jl_value_t **fl_begin, jl_value_t **fl_end) JL_NOTSAFEPOINT;\n void gc_mark_finlist(jl_gc_markqueue_t *mq, arraylist_t *list, size_t start) JL_NOTSAFEPOINT;\n void gc_collect_neighbors(jl_ptls_t ptls, jl_gc_markqueue_t *mq) JL_NOTSAFEPOINT;\n-void gc_mark_loop(jl_ptls_t ptls, int mark_loop_initiator) JL_NOTSAFEPOINT;\n-void gc_sweep_pool_parallel(jl_ptls_t ptls);\n-void gc_free_pages(void);\n-void sweep_stack_pool_loop(void) JL_NOTSAFEPOINT;\n+void gc_mark_queue_all_roots(jl_ptls_t ptls, jl_gc_markqueue_t *mq);\n void jl_gc_debug_init(void);\n \n-// GC pages\n+// GC permanent allocation\n+extern uv_mutex_t gc_perm_lock;\n \n+// GC pages\n extern uv_mutex_t gc_pages_lock;\n void jl_gc_init_page(void) JL_NOTSAFEPOINT;\n NOINLINE jl_gc_pagemeta_t *jl_gc_alloc_page(void) JL_NOTSAFEPOINT;\n-void jl_gc_free_page(jl_gc_pagemeta_t *p) JL_NOTSAFEPOINT;\n+NOINLINE void jl_gc_free_page(jl_gc_pagemeta_t *p) JL_NOTSAFEPOINT;\n \n // GC debug\n-\n #if defined(GC_TIME) || defined(GC_FINAL_STATS)\n void gc_settime_premark_end(void);\n void gc_settime_postmark_end(void);\n@@ -641,32 +718,26 @@ extern int gc_verifying;\n \n int gc_slot_to_fieldidx(void *_obj, void *slot, jl_datatype_t *vt) JL_NOTSAFEPOINT;\n int gc_slot_to_arrayidx(void *_obj, void *begin) JL_NOTSAFEPOINT;\n-NOINLINE void gc_mark_loop_unwind(jl_ptls_t ptls, jl_gc_markqueue_t *mq, int offset) JL_NOTSAFEPOINT;\n \n #ifdef GC_DEBUG_ENV\n JL_DLLEXPORT extern jl_gc_debug_env_t jl_gc_debug_env;\n int jl_gc_debug_check_other(void);\n-int gc_debug_check_pool(void);\n void jl_gc_debug_print(void);\n void gc_scrub_record_task(jl_task_t *ta) JL_NOTSAFEPOINT;\n void gc_scrub(void);\n #else\n-static inline int jl_gc_debug_check_other(void) JL_NOTSAFEPOINT\n-{\n-    return 0;\n-}\n-static inline int gc_debug_check_pool(void) JL_NOTSAFEPOINT\n+STATIC_INLINE int jl_gc_debug_check_other(void) JL_NOTSAFEPOINT\n {\n     return 0;\n }\n-static inline void jl_gc_debug_print(void) JL_NOTSAFEPOINT\n+STATIC_INLINE void jl_gc_debug_print(void) JL_NOTSAFEPOINT\n {\n }\n-static inline void gc_scrub_record_task(jl_task_t *ta) JL_NOTSAFEPOINT\n+STATIC_INLINE void gc_scrub_record_task(jl_task_t *ta) JL_NOTSAFEPOINT\n {\n     (void)ta;\n }\n-static inline void gc_scrub(void) JL_NOTSAFEPOINT\n+STATIC_INLINE void gc_scrub(void) JL_NOTSAFEPOINT\n {\n }\n #endif\n@@ -679,14 +750,6 @@ void gc_stats_big_obj(void);\n #define gc_stats_big_obj()\n #endif\n \n-// For debugging\n-void gc_count_pool(void);\n-\n-JL_DLLEXPORT void jl_enable_gc_logging(int enable);\n-JL_DLLEXPORT int jl_is_gc_logging_enabled(void);\n-JL_DLLEXPORT uint32_t jl_get_num_stack_mappings(void);\n-void _report_gc_finished(uint64_t pause, uint64_t freed, int full, int recollect, int64_t live_bytes) JL_NOTSAFEPOINT;\n-\n #ifdef __cplusplus\n }\n #endif"
    }
  ]
}
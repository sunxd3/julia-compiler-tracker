schema_version: "1.0"

pr:
  number: 60230
  title: "Fix aarch64 macOS crash when SIP disabled (re-land JLJITLinkMemoryManager/#60105)"
  url: "https://github.com/JuliaLang/julia/pull/60230"
  author: "xal-0"
  labels:
    - "re-land"
  merged_at: "2025-11-25T04:21:54Z"
  merge_commit_sha: "068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08"
  diff_url: "https://github.com/JuliaLang/julia/pull/60230.diff"
  related_prs:
    - number: 60105
      title: "Add JLJITLinkMemoryManager (ports memory manager to JITLink)"
      relation: "original PR being re-landed"
    - number: 60196
      title: "Revert 'Add JLJITLinkMemoryManager (ports memory manager to JITLink)'"
      relation: "revert that this PR fixes"

scope:
  files_touched:
    - "src/cgmemmgr.cpp"
    - "src/jitlayers.cpp"
  components:
    - "JITRuntime"
    - "MemoryManagement"
    - "CodeGeneration"
  pipeline_stages:
    - "Codegen"
    - "NativeCodeExecution"

analysis:
  intent:
    summary: |
      This PR re-lands the JLJITLinkMemoryManager from PR #60105, which was reverted in PR #60196
      due to crashes on aarch64 macOS systems with SIP (System Integrity Protection) disabled.

      The root cause of the crash was that Apple ARM CPUs treat the `ic ivau` instruction
      (instruction cache invalidation by VA to PoU) as a memory READ operation. When using
      DualMapAllocator, the write address (`wr_addr`) could be mprotected to `PROT_NONE`
      (no permissions), causing a crash when LLVM's InvalidateInstructionCache tried to
      invalidate cache lines on that address.

      The fix modifies ROAllocator::finalize() to only invalidate instruction cache on the
      runtime address (`rt_addr`), which always has read permission, rather than both addresses.

      This enables memory-efficient JIT allocation via DualMapAllocator on:
      - Apple ARM64 macOS with SIP disabled
      - Linux aarch64
      - RISC-V platforms
    issue_links:
      - "https://discourse.julialang.org/t/ci-testing-hangs-on-macos-nightly/133909"
      - "https://github.com/julia-actions/julia-runtest/pull/155"

  direct_changes:
    - summary: "Fix instruction cache invalidation to only use runtime address"
      component: "MemoryManagement"
      evidence:
        - source: "diff"
          path: "src/cgmemmgr.cpp"
          loc: "548-559"
          url: "https://github.com/JuliaLang/julia/blob/068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08/src/cgmemmgr.cpp#L548-L559"
          snippet: |
            virtual void finalize() JL_NOTSAFEPOINT
            {
                // Note: on some aarch64 platforms, like Apple CPUs, we need read
                // permission in order to invalidate instruction cache lines.  We are
                // not guaranteed to have read permission on the wr_addr when using
                // DualMapAllocator.
                for (auto &alloc : allocations)
                    sys::Memory::InvalidateInstructionCache(alloc.rt_addr, alloc.sz);
                completed.clear();
                allocations.clear();
            }

    - summary: "Re-add JLJITLinkMemoryManager class for JITLink memory management"
      component: "JITRuntime"
      evidence:
        - source: "code"
          path: "src/cgmemmgr.cpp"
          loc: "932-993"
          url: "https://github.com/JuliaLang/julia/blob/068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08/src/cgmemmgr.cpp#L932-L993"
          snippet: |
            class JLJITLinkMemoryManager : public jitlink::JITLinkMemoryManager {
                using OnFinalizedFunction =
                    jitlink::JITLinkMemoryManager::InFlightAlloc::OnFinalizedFunction;

                std::mutex Mutex;
                RWAllocator RWAlloc;
                std::unique_ptr<ROAllocator> ROAlloc;
                std::unique_ptr<ROAllocator> ExeAlloc;
                SmallVector<OnFinalizedFunction> FinalizedCallbacks;
                uint32_t InFlight{0};

            public:
                class InFlightAlloc;

                static std::unique_ptr<JITLinkMemoryManager> Create()
                {
                    auto [ROAlloc, ExeAlloc] = get_preferred_allocators();
                    if (ROAlloc && ExeAlloc)
                        return std::unique_ptr<JLJITLinkMemoryManager>(
                            new JLJITLinkMemoryManager(std::move(ROAlloc), std::move(ExeAlloc)));

                    return cantFail(
                        orc::MapperJITLinkMemoryManager::CreateWithMapper<orc::InProcessMemoryMapper>(
                            /*Reservation Granularity*/ 16 * 1024 * 1024));
                }

    - summary: "Add get_preferred_allocators() factory function"
      component: "MemoryManagement"
      evidence:
        - source: "code"
          path: "src/cgmemmgr.cpp"
          loc: "771-785"
          url: "https://github.com/JuliaLang/julia/blob/068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08/src/cgmemmgr.cpp#L771-L785"
          snippet: |
            std::pair<std::unique_ptr<ROAllocator>, std::unique_ptr<ROAllocator>>
            get_preferred_allocators() JL_NOTSAFEPOINT
            {
            #ifdef _OS_LINUX_
                if (get_self_mem_fd() != -1)
                    return {std::make_unique<SelfMemAllocator>(false),
                            std::make_unique<SelfMemAllocator>(true)};
            #endif
                if (init_shared_map() != -1)
                    return {std::make_unique<DualMapAllocator>(false),
                            std::make_unique<DualMapAllocator>(true)};
                return {};
            }

    - summary: "Refactor ROAllocator and DualMapAllocator to remove template parameter"
      component: "MemoryManagement"
      evidence:
        - source: "diff"
          path: "src/cgmemmgr.cpp"
          loc: "536-611"
          url: "https://github.com/JuliaLang/julia/blob/068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08/src/cgmemmgr.cpp#L536-L611"
          snippet: |
            class ROAllocator {
            protected:
                static constexpr int nblocks = 8;
                SplitPtrBlock blocks[nblocks];
                SmallVector<SplitPtrBlock, 16> completed;
                virtual void *get_wr_ptr(SplitPtrBlock &block, void *rt_ptr,
                                         size_t size, size_t align) JL_NOTSAFEPOINT = 0;
                virtual SplitPtrBlock alloc_block(size_t size) JL_NOTSAFEPOINT = 0;
            public:
                ROAllocator() JL_NOTSAFEPOINT = default;
                virtual ~ROAllocator() JL_NOTSAFEPOINT {}
                virtual void finalize() JL_NOTSAFEPOINT
                {
                    // Note: on some aarch64 platforms, like Apple CPUs, we need read
                    // permission in order to invalidate instruction cache lines.
                    for (auto &alloc : allocations)
                        sys::Memory::InvalidateInstructionCache(alloc.rt_addr, alloc.sz);
                    completed.clear();
                    allocations.clear();
                }
                SmallVector<Allocation, 16> allocations;
                Allocation alloc(size_t size, size_t align) JL_NOTSAFEPOINT;
            };

            class DualMapAllocator : public ROAllocator {
                bool exec;
            protected:
                void *get_wr_ptr(SplitPtrBlock &block, void *rt_ptr, size_t, size_t) override;
                // ...
            public:
                DualMapAllocator(bool exec) JL_NOTSAFEPOINT : exec(exec);
            };

    - summary: "Change RWAllocator and ROAllocator to return Allocation struct"
      component: "MemoryManagement"
      evidence:
        - source: "code"
          path: "src/cgmemmgr.cpp"
          loc: "467-497"
          url: "https://github.com/JuliaLang/julia/blob/068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08/src/cgmemmgr.cpp#L467-L497"
          snippet: |
            struct Allocation {
                // Address to write to (the one returned by the allocation function)
                void *wr_addr;
                // Runtime address
                void *rt_addr;
                size_t sz;
                bool relocated;
            };

            class RWAllocator {
                static constexpr int nblocks = 8;
                Block blocks[nblocks]{};
            public:
                RWAllocator() JL_NOTSAFEPOINT = default;
                Allocation alloc(size_t size, size_t align) JL_NOTSAFEPOINT
                {
                    size_t min_size = (size_t)-1;
                    int min_id = 0;
                    for (int i = 0;i < nblocks && blocks[i].ptr;i++) {
                        if (void *ptr = blocks[i].alloc(size, align))
                            return {ptr, ptr, size, false};
                        if (blocks[i].avail < min_size) {
                            min_size = blocks[i].avail;
                            min_id = i;
                        }
                    }
                    size_t block_size = get_block_size(size);
                    blocks[min_id].reset(map_anon_page(block_size), block_size);
                    void *ptr = blocks[min_id].alloc(size, align);
                    return {ptr, ptr, size, false};
                }
            };

    - summary: "Add JLJITLinkMemoryManager::allocate implementation"
      component: "JITRuntime"
      evidence:
        - source: "code"
          path: "src/cgmemmgr.cpp"
          loc: "1035-1071"
          url: "https://github.com/JuliaLang/julia/blob/068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08/src/cgmemmgr.cpp#L1035-L1071"
          snippet: |
            void JLJITLinkMemoryManager::allocate(const jitlink::JITLinkDylib *JD,
                                                  jitlink::LinkGraph &G,
                                                  OnAllocatedFunction OnAllocated)
            {
                jitlink::BasicLayout BL{G};

                {
                    std::unique_lock Lock{Mutex};
                    for (auto &[AG, Seg] : BL.segments()) {
                        if (AG.getMemLifetime() == orc::MemLifetime::NoAlloc)
                            continue;
                        assert(AG.getMemLifetime() == orc::MemLifetime::Standard);

                        auto Prot = AG.getMemProt();
                        uint64_t Alignment = Seg.Alignment.value();
                        uint64_t Size = Seg.ContentSize + Seg.ZeroFillSize;
                        Allocation Alloc;
                        if (Prot == (MemProt::Read | MemProt::Write))
                            Alloc = RWAlloc.alloc(Size, Alignment);
                        else if (Prot == MemProt::Read)
                            Alloc = ROAlloc->alloc(Size, Alignment);
                        else if (Prot == (MemProt::Read | MemProt::Exec))
                            Alloc = ExeAlloc->alloc(Size, Alignment);
                        else
                            abort();

                        Seg.Addr = orc::ExecutorAddr::fromPtr(Alloc.rt_addr);
                        Seg.WorkingMem = (char *)Alloc.wr_addr;
                    }
                }

                if (auto Err = BL.apply())
                    return OnAllocated(std::move(Err));

                ++InFlight;
                OnAllocated(std::make_unique<InFlightAlloc>(*this, G));
            }

    - summary: "Move createJITLinkMemoryManager to cgmemmgr.cpp"
      component: "JITRuntime"
      evidence:
        - source: "diff"
          path: "src/jitlayers.cpp"
          loc: "1237"
          url: "https://github.com/JuliaLang/julia/blob/068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08/src/jitlayers.cpp#L1237"
          snippet: |
            RTDyldMemoryManager *createRTDyldMemoryManager(void) JL_NOTSAFEPOINT;
            std::unique_ptr<jitlink::JITLinkMemoryManager> createJITLinkMemoryManager() JL_NOTSAFEPOINT;

  secondary_effects:
    - effect: "Memory savings on platforms supporting DualMapAllocator"
      mechanism: |
        The DualMapAllocator uses shared memory mappings to avoid duplicating JIT code in memory.
        It creates two views of the same physical memory:
        1. A writable view (wr_addr) for writing JIT code
        2. An executable view (rt_addr) for executing the code

        This is more memory-efficient than the default MapperJITLinkMemoryManager which reserves
        large contiguous regions (16MB granularity).

        Call chain:
        createJITLinkMemoryManager() [cgmemmgr.cpp:1069-1072]
          -> JLJITLinkMemoryManager::Create() [cgmemmgr.cpp:944-954]
          -> get_preferred_allocators() [cgmemmgr.cpp:771-783]
          -> DualMapAllocator(exec) [cgmemmgr.cpp:676]
          -> Uses shared file mapping via init_shared_map() [cgmemmgr.cpp:259-272]

        On Linux aarch64 with /proc/self/mem available, uses SelfMemAllocator instead.
      downstream_surfaces:
        - "JIT memory usage on aarch64 macOS with SIP disabled"
        - "JIT memory usage on Linux aarch64"
        - "JIT memory usage on RISC-V"
      likelihood: "high"
      impact: "medium"

    - effect: "Potential for hangs eliminated on Apple ARM with SIP disabled"
      mechanism: |
        The original bug was caused by Apple ARM's cache invalidation instruction behavior.
        CRITICAL: The bug manifested as a HANG, not just a crash, because the SIGBUS
        occurred while holding the JLJITLinkMemoryManager::Mutex, leaving other threads
        blocked indefinitely.

        Precise sequence of operations leading to crash:

        1. JLJITLinkMemoryManager::allocate() is called [cgmemmgr.cpp:1020-1056]
           - Allocates memory via ExeAlloc->alloc() for executable segments
           - Returns Allocation with both wr_addr and rt_addr
           - Increments InFlight counter: ++InFlight [cgmemmgr.cpp:1054]

        2. InFlightAlloc::finalize() is called [cgmemmgr.cpp:1002-1015]
           - Calls JLJITLinkMemoryManager::finalize() with callback

        3. JLJITLinkMemoryManager::finalize() acquires Mutex [cgmemmgr.cpp:972-989]
           - std::unique_lock Lock{Mutex};
           - When --InFlight reaches 0, proceeds with finalization

        4. DualMapAllocator::finalize() is called [cgmemmgr.cpp:680-690]
           - Calls finalize_block() for each block

        5. finalize_block() removes permissions from wr_ptr [cgmemmgr.cpp:666-673]:
           protect_page((void*)block.wr_ptr, block.total, Prot::NO);
           This sets PROT_NONE on wr_addr, making it inaccessible.

        6. ROAllocator::finalize() is called [cgmemmgr.cpp:689]
           - STILL HOLDING THE MUTEX

        7. OLD BUGGY CODE [from PR #60105] attempted to invalidate on BOTH addresses:
           for (auto &alloc: allocations) {
               sys::Memory::InvalidateInstructionCache(alloc.wr_addr, alloc.sz);  // SIGBUS!
               sys::Memory::InvalidateInstructionCache(alloc.rt_addr, alloc.sz);
           }

        8. On Apple ARM, InvalidateInstructionCache uses `ic ivau` instruction which
           ARM documents as performing a memory READ. Since wr_addr now has PROT_NONE,
           this causes SIGBUS.

        9. The crash happens WHILE HOLDING THE MUTEX, so any other thread waiting on
           the mutex will hang indefinitely - explaining the CI timeout behavior.

        The fix changes finalize() to only invalidate on rt_addr [cgmemmgr.cpp:555-556]:
           for (auto &alloc : allocations)
               sys::Memory::InvalidateInstructionCache(alloc.rt_addr, alloc.sz);

        rt_addr always has read permission (RO or RX) and refers to the same physical
        memory as wr_addr, so cache invalidation is still effective.
      downstream_surfaces:
        - "Julia startup on aarch64 macOS with SIP disabled"
        - "Julia CI/CD on GitHub Actions aarch64 macOS runners"
      likelihood: "high"
      impact: "high"

    - effect: "JITLink integration uses Julia's optimized memory allocators"
      mechanism: |
        JuliaOJIT constructor now uses JLJITLinkMemoryManager for JITLink.

        JL_USE_JITLINK is automatically enabled for [jitlayers.h:58-64]:
        - _CPU_AARCH64_ (all ARM64 platforms)
        - _CPU_RISCV64_ (all RISC-V 64-bit platforms)
        - HAS_SANITIZER (when address/memory sanitizers are enabled)
        - JL_FORCE_JITLINK (manual override)

        src/jitlayers.cpp:1922-1926:
        #ifdef JL_USE_JITLINK
            MemMgr(createJITLinkMemoryManager()),
            ObjectLayer(ES, *MemMgr),
        #else
            MemMgr(createRTDyldMemoryManager()),

        JLJITLinkMemoryManager::Create() returns either:
        1. JLJITLinkMemoryManager using SelfMemAllocator (Linux with /proc/self/mem)
        2. JLJITLinkMemoryManager using DualMapAllocator (macOS/FreeBSD with shared mem)
        3. MapperJITLinkMemoryManager fallback (when preferred allocators unavailable)

        This replaces the previous behavior where JITLink always used the less efficient
        MapperJITLinkMemoryManager with 16MB reservation granularity.
      downstream_surfaces:
        - "JIT compilation memory efficiency"
        - "Systems using JITLink backend"
      likelihood: "high"
      impact: "medium"

    - effect: "Linux aarch64 unaffected because SelfMemAllocator does not use Prot::NO"
      mechanism: |
        On Linux, SelfMemAllocator is preferred over DualMapAllocator when /proc/self/mem
        is available [cgmemmgr.cpp:774-777]:

        #ifdef _OS_LINUX_
            if (get_self_mem_fd() != -1)
                return {std::make_unique<SelfMemAllocator>(false),
                        std::make_unique<SelfMemAllocator>(true)};
        #endif

        SelfMemAllocator::finalize_block() [cgmemmgr.cpp:719-730] does NOT call
        protect_page with Prot::NO on wr_ptr. Instead, it uses temporary buffers
        and writes directly via /proc/self/mem:

        for (auto &alloc : this->allocations) {
            if (alloc.rt_addr == alloc.wr_addr)
                continue;
            write_self_mem(alloc.rt_addr, alloc.wr_addr, alloc.sz);
        }

        The wr_addr in SelfMemAllocator points to regular anonymous memory that
        maintains RW permissions, so InvalidateInstructionCache on wr_addr would
        not crash. However, the fix to only use rt_addr is still correct and more
        robust for all allocator implementations.
      downstream_surfaces:
        - "Linux aarch64 JIT behavior"
      likelihood: "high"
      impact: "low"

    - effect: "JIT code deallocation not supported - potential memory concern for long sessions"
      mechanism: |
        The JLJITLinkMemoryManager does NOT support deallocating JIT code:

        void deallocate(std::vector<FinalizedAlloc> Allocs,
                        OnDeallocatedFunction OnDeallocated) override
        {
            jl_unreachable();  // [cgmemmgr.cpp:959-963]
        }

        void abandon(OnAbandonedFunction OnAbandoned) override { jl_unreachable(); }
        // [cgmemmgr.cpp:1000]

        This means JIT-compiled code memory is never freed during the lifetime of
        a Julia session. For long-running sessions that generate significant
        amounts of JIT code (e.g., interactive REPL sessions, Jupyter notebooks),
        this could lead to increased memory usage over time.

        NOTE: This is the same behavior as the RTDyld-based allocators and is not
        a regression introduced by this PR.
      downstream_surfaces:
        - "Long-running Julia sessions memory usage"
        - "Julia REPL and Jupyter notebook sessions"
      likelihood: "medium"
      impact: "low"

    - effect: "Thread-safe batching of allocations before finalization"
      mechanism: |
        JLJITLinkMemoryManager uses an InFlight counter to batch multiple allocations
        before performing finalization [cgmemmgr.cpp:972-989]:

        void finalize(OnFinalizedFunction OnFinalized)
        {
            SmallVector<OnFinalizedFunction> Callbacks;
            {
                std::unique_lock Lock{Mutex};
                FinalizedCallbacks.push_back(std::move(OnFinalized));

                if (--InFlight > 0)
                    return;  // More allocations pending, don't finalize yet

                ROAlloc->finalize();
                ExeAlloc->finalize();
                Callbacks = std::move(FinalizedCallbacks);
            }

            for (auto &CB : Callbacks)
                std::move(CB)(FinalizedAlloc{});
        }

        This batching is important because finalization invalidates all wr_addr
        pointers (by removing permissions). Multiple concurrent JIT compilations
        can allocate memory, and finalization only happens when ALL in-flight
        allocations complete.
      downstream_surfaces:
        - "Concurrent JIT compilation behavior"
      likelihood: "high"
      impact: "low"

  compatibility:
    internal_api:
      - field: "ROAllocator template parameter removed"
        change: "ROAllocator<bool exec> is now ROAllocator with exec passed to constructor"
        affected_tools: []
      - field: "RWAllocator::alloc() return type"
        change: "Returns Allocation struct instead of void*"
        affected_tools: []
      - field: "ROAllocator::alloc() return type"
        change: "Returns Allocation struct instead of void*"
        affected_tools: []
      - field: "createJITLinkMemoryManager() implementation"
        change: "Now returns JLJITLinkMemoryManager when preferred allocators available"
        affected_tools: []
    behavioral:
      - field: "Instruction cache invalidation behavior"
        change: "Only invalidates on rt_addr, not wr_addr; same physical memory, no functional change"
        affected_tools: []

  performance:
    compile_time:
      - impact: "Negligible"
        details: |
          The changes do not affect compilation time. The memory allocation strategy
          is the same as the previous RTDyld-based approach. JITLink linking time may
          be slightly improved due to better memory locality with the optimized allocators.
    runtime:
      - impact: "Potential memory reduction for JIT code"
        details: |
          ESTIMATED: Up to 50% reduction in JIT memory overhead on platforms supporting
          DualMapAllocator compared to the default 16MB granularity MapperJITLinkMemoryManager.

          The DualMapAllocator uses smaller allocation blocks (256 pages per block, with
          up to 8 blocks tracked) and shares physical memory between write and execute views.

          Actual savings depend on:
          - Platform (Linux aarch64 vs macOS ARM64 vs x86_64)
          - SIP status on macOS (must be disabled for DualMapAllocator)
          - JIT workload size and fragmentation

  risk:
    level: "low"
    rationale:
      - "The fix is minimal - changing which address is used for cache invalidation"
      - "The rt_addr and wr_addr point to the same physical memory, just different virtual mappings"
      - "Falls back to MapperJITLinkMemoryManager when optimized allocators unavailable"
      - "Already tested in PR #60105 before the revert, now with the crash fix"
      - "Author (xal-0) is the same developer who wrote the original implementation"
      - "The revert/re-land cycle provides additional validation through bug discovery"

  open_questions:
    - question: "Why didn't the crash occur on Linux aarch64?"
      resolved: true
      answer: |
        On Linux, the SelfMemAllocator is preferred over DualMapAllocator when
        /proc/self/mem is available (cgmemmgr.cpp:774-777). SelfMemAllocator uses
        a different mechanism that writes directly through /proc/self/mem rather
        than using dual memory mappings.

        Critically, SelfMemAllocator::finalize_block() does NOT call protect_page()
        with Prot::NO on wr_ptr - it only sets permissions on the runtime address.
        The temporary write buffers maintain RW permissions throughout.

    - question: "Why does this only affect systems with SIP disabled?"
      resolved: true
      answer: |
        When SIP (System Integrity Protection) is enabled on macOS, creating executable
        shared memory mappings is restricted. The init_shared_map() -> get_anon_hdl()
        chain attempts several methods to create executable shared memory:
        1. memfd_create (Linux only)
        2. shm_open with SHM_ANON (FreeBSD)
        3. shm_open (not macOS - documented: "shm_open can't be mapped exec on mac")
        4. tmpfile() with dup()
        5. mkstemp() as last resort

        On macOS, all these methods fail when SIP is enabled because
        check_fd_or_close() [cgmemmgr.cpp:105-126] tests if the fd can be mapped
        with PROT_READ | PROT_EXEC. SIP blocks this for shared memory.

        When init_shared_map() returns -1, get_preferred_allocators() returns empty
        and the fallback MapperJITLinkMemoryManager is used instead.

    - question: "Could this affect x86_64 platforms?"
      resolved: true
      answer: |
        No, for two reasons:
        1. x86_64 does NOT use JITLink by default. JL_USE_JITLINK is only defined
           for _CPU_AARCH64_, _CPU_RISCV64_, or when HAS_SANITIZER is defined
           [jitlayers.h:58-64].
        2. Even if x86_64 used JITLink with DualMapAllocator, the x86 cache
           invalidation instruction (clflush) does not perform a memory read
           in the same way as ARM's `ic ivau`.

    - question: "Why did the bug manifest as a hang rather than a crash?"
      resolved: true
      answer: |
        The SIGBUS occurred inside JLJITLinkMemoryManager::finalize() while holding
        the Mutex [cgmemmgr.cpp:976]. The sequence is:

        1. std::unique_lock Lock{Mutex}; - Acquires mutex
        2. ... finalization proceeds ...
        3. DualMapAllocator::finalize() -> finalize_block() sets Prot::NO
        4. ROAllocator::finalize() -> InvalidateInstructionCache(wr_addr) -> SIGBUS
        5. Crash happens WHILE HOLDING MUTEX

        Any other thread trying to allocate JIT memory would call
        JLJITLinkMemoryManager::allocate() which also acquires the same Mutex
        [cgmemmgr.cpp:1027]. Since the mutex is never released (due to the crash),
        these threads block indefinitely, causing the "hang" behavior seen in CI.

    - question: "Is the fix correct - doesn't cache invalidation need both addresses?"
      resolved: true
      answer: |
        The fix is correct. wr_addr and rt_addr are two VIRTUAL address mappings
        of the SAME PHYSICAL memory. Cache invalidation operates on physical
        addresses internally - the CPU translates the virtual address to physical.

        Since both mappings point to the same physical memory, invalidating the
        cache using either virtual address achieves the same result. Using rt_addr
        is preferred because:
        1. rt_addr always has at least read permission (RO or RX)
        2. rt_addr is the address that will actually be used for code execution
        3. No functional difference in cache behavior

  recommendations:
    - "No action required for downstream packages"
    - "This is a low-level JIT runtime fix with no user-facing API changes"
    - "Julia users on aarch64 macOS with SIP disabled should benefit from improved memory usage"
    - "Consider monitoring JIT memory usage to confirm memory savings on affected platforms"

changelog_entry:
  category: "JIT/Runtime"
  breaking: false
  summary: |
    Fixed a crash on Apple ARM64 macOS systems with SIP disabled when using the
    JLJITLinkMemoryManager. The issue was caused by the instruction cache invalidation
    instruction (`ic ivau`) performing a memory read on an address with no permissions.
    Also re-enables memory-efficient JIT allocation on affected platforms.
  downstream_impact: |
    This is a transparent bug fix and performance improvement for the JIT runtime.
    No changes to Julia semantics or compiler behavior. Users on aarch64 macOS with
    SIP disabled will see Julia startup work correctly again, with potential memory
    savings from the optimized memory allocator.

downstream_package_impact:
  Turing_jl: "none - transparent JIT runtime fix, no Julia-level API changes"
  Enzyme_jl: "none - transparent JIT runtime fix, no Julia-level API changes"
  GPUCompiler: "none - transparent JIT runtime fix, GPUCompiler uses its own codegen path"
  JET: "none - transparent JIT runtime fix, no compiler semantics changed"
  IRTools: "none - transparent JIT runtime fix, no IR representation changes"
  Cassette: "none - transparent JIT runtime fix, no compiler semantics changed"

code_path_trace:
  cache_invalidation_fix:
    description: "The core fix for the Apple ARM crash"
    steps:
      - location: "src/cgmemmgr.cpp:548-559"
        url: "https://github.com/JuliaLang/julia/blob/068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08/src/cgmemmgr.cpp#L548-L559"
        code: |
          virtual void finalize() JL_NOTSAFEPOINT
          {
              // Note: on some aarch64 platforms, like Apple CPUs, we need read
              // permission in order to invalidate instruction cache lines.  We are
              // not guaranteed to have read permission on the wr_addr when using
              // DualMapAllocator.
              for (auto &alloc : allocations)
                  sys::Memory::InvalidateInstructionCache(alloc.rt_addr, alloc.sz);
              completed.clear();
              allocations.clear();
          }
        explanation: |
          The key fix: only invalidate instruction cache on rt_addr (runtime address)
          which always has at least read permission. The wr_addr may have been
          mprotected to PROT_NONE by DualMapAllocator::finalize_block(), which would
          cause a crash on Apple ARM CPUs when trying to invalidate.

  dual_map_finalize_block:
    description: "Where wr_addr permissions are removed"
    steps:
      - location: "src/cgmemmgr.cpp:666-673"
        url: "https://github.com/JuliaLang/julia/blob/068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08/src/cgmemmgr.cpp#L666-L673"
        code: |
          else {
              assert(block.state & SplitPtrBlock::WRInit);
              assert(block.state & SplitPtrBlock::WRReady);
              if (reset) {
                  unmap_page((void*)block.wr_ptr, block.total);
              }
              else {
                  protect_page((void*)block.wr_ptr, block.total, Prot::NO);
                  block.state = SplitPtrBlock::WRInit;
              }
          }
        explanation: |
          When not resetting, DualMapAllocator::finalize_block() calls protect_page
          with Prot::NO (PROT_NONE) on the write pointer. This removes all permissions
          from the wr_addr mapping. After this, trying to access wr_addr (even for
          cache invalidation) will cause a fault on Apple ARM.

  jitlink_memory_manager_creation:
    description: "How JLJITLinkMemoryManager is instantiated"
    steps:
      - location: "src/jitlayers.cpp:1931-1932"
        url: "https://github.com/JuliaLang/julia/blob/068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08/src/jitlayers.cpp#L1931-L1932"
        code: |
          #ifdef JL_USE_JITLINK
              MemMgr(createJITLinkMemoryManager()),
              ObjectLayer(ES, *MemMgr),
        explanation: |
          JuliaOJIT constructor initializes MemMgr by calling createJITLinkMemoryManager()
          when JL_USE_JITLINK is defined. This returns the new JLJITLinkMemoryManager.

      - location: "src/cgmemmgr.cpp:1084-1087"
        url: "https://github.com/JuliaLang/julia/blob/068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08/src/cgmemmgr.cpp#L1084-L1087"
        code: |
          std::unique_ptr<jitlink::JITLinkMemoryManager> createJITLinkMemoryManager()
          {
              return JLJITLinkMemoryManager::Create();
          }
        explanation: |
          createJITLinkMemoryManager() delegates to JLJITLinkMemoryManager::Create()
          which either returns a JLJITLinkMemoryManager with optimized allocators
          or falls back to MapperJITLinkMemoryManager.

      - location: "src/cgmemmgr.cpp:946-956"
        url: "https://github.com/JuliaLang/julia/blob/068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08/src/cgmemmgr.cpp#L946-L956"
        code: |
          static std::unique_ptr<JITLinkMemoryManager> Create()
          {
              auto [ROAlloc, ExeAlloc] = get_preferred_allocators();
              if (ROAlloc && ExeAlloc)
                  return std::unique_ptr<JLJITLinkMemoryManager>(
                      new JLJITLinkMemoryManager(std::move(ROAlloc), std::move(ExeAlloc)));

              return cantFail(
                  orc::MapperJITLinkMemoryManager::CreateWithMapper<orc::InProcessMemoryMapper>(
                      /*Reservation Granularity*/ 16 * 1024 * 1024));
          }
        explanation: |
          Create() first tries to get preferred allocators (SelfMemAllocator on Linux,
          DualMapAllocator on macOS/FreeBSD). If successful, creates JLJITLinkMemoryManager.
          Otherwise falls back to the default LLVM memory manager with 16MB granularity.

  allocator_selection:
    description: "How the preferred allocator is selected"
    steps:
      - location: "src/cgmemmgr.cpp:771-785"
        url: "https://github.com/JuliaLang/julia/blob/068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08/src/cgmemmgr.cpp#L771-L785"
        code: |
          std::pair<std::unique_ptr<ROAllocator>, std::unique_ptr<ROAllocator>>
          get_preferred_allocators() JL_NOTSAFEPOINT
          {
          #ifdef _OS_LINUX_
              if (get_self_mem_fd() != -1)
                  return {std::make_unique<SelfMemAllocator>(false),
                          std::make_unique<SelfMemAllocator>(true)};
          #endif
              if (init_shared_map() != -1)
                  return {std::make_unique<DualMapAllocator>(false),
                          std::make_unique<DualMapAllocator>(true)};
              return {};
          }
        explanation: |
          Priority order:
          1. Linux with /proc/self/mem available: use SelfMemAllocator
          2. Shared memory mapping available: use DualMapAllocator
          3. Neither available: return empty, triggering fallback to MapperJITLinkMemoryManager

          The first argument (false/true) indicates whether the allocator is for
          executable (true) or read-only (false) memory.

test_coverage:
  existing_tests: "No specific test changes in this PR"
  rationale: |
    This is a bug fix for a crash that was discovered in CI environments on aarch64 macOS
    with SIP disabled. The bug manifested as a hang at startup (mutex wait after SIGBUS).

    Testing this fix requires:
    1. An aarch64 macOS system with SIP disabled
    2. Running Julia startup (the crash occurred during JIT initialization)

    The fix is validated by:
    1. The CI environments that previously hung now work
    2. The existing Julia test suite passes on affected platforms

reviewer_notes:
  initial_reviewer: "automated_analysis"
  initial_date: "2026-01-21"
  verification_method: |
    1. Read PR metadata from pr-archive/JuliaLang_julia/pr_60230.json
    2. Read related PRs: #60105 (original), #60196 (revert)
    3. Read full source of src/cgmemmgr.cpp to understand the memory manager architecture
    4. Traced the instruction cache invalidation code path
    5. Analyzed the DualMapAllocator finalization logic that sets Prot::NO on wr_addr
    6. Verified the fix changes only the address used for cache invalidation
    7. Traced JITLink integration through jitlayers.cpp
  findings:
    - "The crash was caused by ARM's `ic ivau` instruction reading from PROT_NONE memory"
    - "Fix is minimal: use rt_addr instead of wr_addr for cache invalidation"
    - "Both addresses point to same physical memory, so invalidation is still effective"
    - "Falls back safely to MapperJITLinkMemoryManager when optimized allocators unavailable"
    - "The revert/re-land cycle provides good validation of the bug fix"
  confidence: "high"
  rationale: |
    The analysis is based on direct examination of the source code and understanding of:
    1. ARM instruction cache invalidation semantics
    2. Memory protection and dual mapping architecture
    3. LLVM JITLink memory management interfaces

    The fix is straightforward and low-risk since it only changes which virtual address
    is used for cache invalidation, while the physical memory remains the same.

  secondary_review:
    reviewer: "independent_analysis"
    date: "2026-01-21"
    method: |
      1. Checked out Julia repo at merge commit 068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08
      2. Read complete source files src/cgmemmgr.cpp (1073 lines) and src/jitlayers.cpp
      3. Traced jitlayers.h to understand JL_USE_JITLINK platform conditions
      4. Analyzed related PRs #60105 and #60196 for full context
      5. Verified the exact timing/ordering of operations leading to the bug
      6. Searched for callers using rg to verify no missed code paths
    additional_findings:
      - "Bug manifested as HANG because SIGBUS occurred while holding Mutex - other threads blocked"
      - "JL_USE_JITLINK is default-enabled on aarch64, riscv64, and with sanitizers"
      - "SelfMemAllocator on Linux is NOT affected - does not use Prot::NO on wr_ptr"
      - "deallocate() and abandon() call jl_unreachable() - JIT memory never freed"
      - "InFlight counter batches multiple allocations before finalization"
      - "The fix is architecturally sound - virtual addresses map to same physical memory"
    enhancements_made:
      - "Added detailed bug sequence showing exact timing and mutex holding"
      - "Added platform coverage showing which architectures use JITLink"
      - "Added secondary effect about memory deallocation not being supported"
      - "Added secondary effect about thread-safe batching mechanism"
      - "Added explanation of why Linux aarch64 was unaffected"
      - "Added new open question about why hang vs crash"
      - "Added new open question about cache invalidation correctness"
    agreement_with_initial: |
      The initial analysis was accurate and comprehensive. The secondary review
      confirms all findings and adds deeper detail about:
      1. The exact sequence of operations and timing
      2. Why the bug manifested as a hang specifically
      3. Platform-specific details (JL_USE_JITLINK conditions)
      4. Memory management limitations (no deallocation)
    confidence: "high"

  tertiary_review:
    reviewer: "claude_opus_4_independent"
    date: "2026-01-22"
    method: |
      1. Read existing analysis to understand first analyst's findings
      2. Independently checked out Julia repo at merge commit 068186c191fc3d0ba7a1cf54ee36ae4ff55c1c08
      3. Read complete src/cgmemmgr.cpp (1073 lines) and src/jitlayers.h
      4. Compared original PR #60105, revert PR #60196, and fix PR #60230
      5. Traced all callers of InvalidateInstructionCache, finalize(), and protect_page()
      6. Verified the root cause by comparing 60105 vs 60230 code changes
    key_verification: |
      CONFIRMED: The critical difference between PR #60105 (buggy) and PR #60230 (fixed) is:

      Original PR #60105 ROAllocator::finalize() [cgmemmgr.cpp:548-557]:
        for (auto &alloc: allocations) {
            sys::Memory::InvalidateInstructionCache(alloc.wr_addr, alloc.sz);  // BUG: wr_addr
            sys::Memory::InvalidateInstructionCache(alloc.rt_addr, alloc.sz);
        }

      Fixed PR #60230 ROAllocator::finalize() [cgmemmgr.cpp:549-559]:
        // Note: on some aarch64 platforms, like Apple CPUs, we need read
        // permission in order to invalidate instruction cache lines.
        for (auto &alloc : allocations)
            sys::Memory::InvalidateInstructionCache(alloc.rt_addr, alloc.sz);  // FIX: only rt_addr
    timeline_verification: |
      1. 2025-11-13: PR #60105 merged - Added JLJITLinkMemoryManager with bug
      2. 2025-11-21 to 2025-11-22: Bug discovered on macOS aarch64 with SIP disabled
      3. 2025-11-22: PR #60196 merged - Reverted PR #60105 due to startup hang
      4. 2025-11-25: PR #60230 merged - Re-landed with fix (only invalidate rt_addr)
    hang_evidence_from_revert_pr: |
      From PR #60196 body, the exact error when SIGTERM was sent to the hanging process:
        in expression starting at none:0
        __psynch_cvwait at /usr/lib/system/libsystem_kernel.dylib (unknown line)
        unknown function (ip: 0x0) at (unknown file)
        __psynch_mutexwait at /usr/lib/system/libsystem_kernel.dylib (unknown line)
        unknown function (ip: 0x0) at (unknown file)
        Allocations: 1 (Pool: 1; Big: 0); GC: 0

      The __psynch_mutexwait and __psynch_cvwait calls confirm threads were blocked waiting
      on mutexes/condition variables, validating the "hang" diagnosis in the existing analysis.
    additional_findings:
      - "The comment added in the fix is excellent documentation of the platform-specific behavior"
      - "The fix preserves the architectural invariant that both addresses map to same physical memory"
      - "SIP status determines DualMapAllocator availability via check_fd_or_close() test [cgmemmgr.cpp:117-119] which tries PROT_READ | PROT_EXEC mapping"
      - "The InFlight counter increment (++InFlight) happens AFTER allocation but BEFORE finalize, ensuring all allocations complete before finalization runs"
    code_path_verification: |
      Verified call chain from JuliaOJIT initialization to crash point:

      1. JuliaOJIT::JuliaOJIT() [jitlayers.cpp:1914-1942]
         MemMgr(createJITLinkMemoryManager()) [jitlayers.cpp:1923]

      2. createJITLinkMemoryManager() [cgmemmgr.cpp:1069-1072]
         return JLJITLinkMemoryManager::Create()

      3. JLJITLinkMemoryManager::Create() [cgmemmgr.cpp:944-954]
         auto [ROAlloc, ExeAlloc] = get_preferred_allocators()
         -> Returns DualMapAllocator(false), DualMapAllocator(true) on macOS with SIP disabled

      4. JLJITLinkMemoryManager::allocate() [cgmemmgr.cpp:1020-1056]
         std::unique_lock Lock{Mutex} [cgmemmgr.cpp:1027]
         ExeAlloc->alloc(Size, Alignment) [cgmemmgr.cpp:1042]
         ++InFlight [cgmemmgr.cpp:1054]

      5. InFlightAlloc::finalize() -> JLJITLinkMemoryManager::finalize() [cgmemmgr.cpp:972-989]
         std::unique_lock Lock{Mutex} [cgmemmgr.cpp:976]
         ExeAlloc->finalize() [cgmemmgr.cpp:983]

      6. DualMapAllocator::finalize() [cgmemmgr.cpp:680-690]
         finalize_block(block, false) [cgmemmgr.cpp:683]

      7. DualMapAllocator::finalize_block() [cgmemmgr.cpp:640-674]
         protect_page((void*)block.wr_ptr, block.total, Prot::NO) [cgmemmgr.cpp:670]
         -> wr_addr now has PROT_NONE (no permissions)

      8. ROAllocator::finalize() [cgmemmgr.cpp:689 -> cgmemmgr.cpp:549-559]
         sys::Memory::InvalidateInstructionCache(alloc.rt_addr, alloc.sz) [cgmemmgr.cpp:556]
         -> FIXED: Now only invalidates rt_addr which has read permission

         ORIGINAL BUG in PR #60105:
         sys::Memory::InvalidateInstructionCache(alloc.wr_addr, alloc.sz)
         -> CRASH: wr_addr has PROT_NONE, ARM's ic ivau treats this as a READ -> SIGBUS
         -> HANG: SIGBUS occurs while Mutex is held [cgmemmgr.cpp:976], blocking other threads
    agreement_with_existing_analysis: |
      The existing initial and secondary analyses are accurate and comprehensive.
      My tertiary review independently confirms:
      1. Root cause: ARM ic ivau instruction performs memory READ on PROT_NONE address
      2. Manifestation: HANG (not just crash) due to mutex held during SIGBUS
      3. Fix correctness: rt_addr always has read permission, maps to same physical memory
      4. Platform scope: Only affects macOS aarch64 with SIP disabled (DualMapAllocator path)
      5. Linux aarch64 unaffected: SelfMemAllocator doesn't use Prot::NO on wr_addr
    confidence: "high"
    rationale: |
      Independent code examination confirms all findings. The fix is minimal and correct:
      - Single conceptual change (use rt_addr instead of wr_addr for cache invalidation)
      - No behavioral change for correctly functioning code
      - Proper comment documents the platform-specific ARM behavior
      - Falls back safely to MapperJITLinkMemoryManager when optimized allocators unavailable

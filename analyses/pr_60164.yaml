schema_version: "1.0"
pr:
  number: 60164
  title: "Fix linearly indexed array math by reshaping arrays"
  url: "https://github.com/JuliaLang/julia/pull/60164"
  author: "jishnub"
  labels:
    - "arrays"
    - "bugfix"
  merged_at: "2025-11-26T14:56:22Z"
  merge_commit_sha: "ee6bb20b"
  diff_url: "https://github.com/JuliaLang/julia/pull/60164.diff"
scope:
  files_touched:
    - "base/arraymath.jl"
    - "test/abstractarray.jl"
  components:
    - "Base"
    - "Arrays"
  pipeline_stages:
    - "Runtime"
analysis:
  intent:
    summary: |
      Fix a regression introduced by PR #59961 where `map` was used for array addition/subtraction
      to improve performance. The problem was that `map` handles trailing singleton dimensions
      differently than `broadcast`: map(+, ones(1), ones(1,1)) returns size (1,) while
      broadcast(+, ones(1), ones(1,1)) returns size (1,1). This PR fixes the issue by:
      1. Adding a fast path when all arrays have the same ndims (no reshaping needed)
      2. Adding a fallback that reshapes arrays to match the highest-ndims shape when ndims differ
    issue_links:
      - "https://github.com/JuliaLang/julia/pull/59961"
      - "https://github.com/JuliaLang/julia/issues/47873"
    reviewer_note: |
      PR description from jishnub: "This was broken in #59961, as `map` deals with
      trailing singleton axes differently from broadcasting... This PR limits the
      new method to the case where the ndims match, in which case there are no
      trailing axes and the two are equivalent."
  direct_changes:
    - summary: "Added fast-path method for arrays with matching ndims - uses map directly without reshaping"
      component: "base/arraymath.jl"
      evidence:
        - source: "code"
          path: "base/arraymath.jl"
          loc: "12-14"
          url: "https://github.com/JuliaLang/julia/blob/ee6bb20b/base/arraymath.jl#L12-L14"
          snippet: |
            function _broadcast_preserving_zero_d(f, A::Array{<:Any,N}, B::Array{<:Any,N}, Cs::Array{<:Any,N}...) where {N}
                map(f, A, B, Cs...)
            end
    - summary: "Added fallback method that reshapes arrays when ndims differ - selects highest-ndims shape and reshapes all arrays to match"
      component: "base/arraymath.jl"
      evidence:
        - source: "code"
          path: "base/arraymath.jl"
          loc: "16-25"
          url: "https://github.com/JuliaLang/julia/blob/ee6bb20b/base/arraymath.jl#L16-L25"
          snippet: |
            function _broadcast_preserving_zero_d(f, A::Array, B::Array, Cs::Array...)
                # we already know that the shapes are compatible.
                # We just need to select the size corresponding to the higest ndims
                # and reshape all the arrays to that size
                arrays = (A, B, Cs...)
                sz = mapreduce(size, (x,y) -> length(x) > length(y) ? x : y, arrays)
                # Skip reshaping where possible to avoid the overhead
                arrays_sameshape = map(x -> length(sz) == ndims(x) ? x : reshape(x, sz), arrays)
                map(f, arrays_sameshape...)
            end
    - summary: "Changed multi-argument +(A::Array, B::Array, Cs::Array...) to use _broadcast_preserving_zero_d instead of direct map call"
      component: "base/arraymath.jl"
      evidence:
        - source: "code"
          path: "base/arraymath.jl"
          loc: "42-48"
          url: "https://github.com/JuliaLang/julia/blob/ee6bb20b/base/arraymath.jl#L42-L48"
          snippet: |
            function +(A::Array, B::Array, Cs::Array...)
                promote_shape(A, B)
                for C in Cs
                    promote_shape(A, C) # check size compatibility
                end
                _broadcast_preserving_zero_d(+, A, B, Cs...)
            end
    - summary: "Generic fallback for non-Array types still uses broadcast semantics"
      component: "base/arraymath.jl"
      evidence:
        - source: "code"
          path: "base/arraymath.jl"
          loc: "5-7"
          url: "https://github.com/JuliaLang/julia/blob/ee6bb20b/base/arraymath.jl#L5-L7"
          snippet: |
            function _broadcast_preserving_zero_d(f, A, B)
                broadcast_preserving_zero_d(f, A, B)
            end
    - summary: "Added comprehensive tests for size promotion in array addition/subtraction with mixed ndims"
      component: "test/abstractarray.jl"
      evidence:
        - source: "code"
          path: "test/abstractarray.jl"
          loc: "2380-2391"
          url: "https://github.com/JuliaLang/julia/blob/ee6bb20b/test/abstractarray.jl#L2380-L2391"
          snippet: |
            @testset "size promotion in addition/subtraction" begin
                for A in Any[ones(), ones(1), ones(1,1,1)]
                    @test +(A) == A
                    for B in Any[ones(), ones(1), ones(1,1,1)]
                        sz = ndims(A) > ndims(B) ? size(A) : size(B)
                        @test A + B == fill(2.0,sz)
                        @test A - B == zeros(sz)
                        @test A + B + zeros() == A + B
                        @test A - B - zeros() == A - B
                    end
                end
            end
  secondary_effects:
    - effect: "Output shape of array + and - now consistently follows broadcast semantics even when using map internally"
      mechanism: |
        Call chain for Array addition with different ndims:
        +(A::Array, B::Array)  [arraymath.jl:36-39]
          -> promote_shape(A, B)  [indices.jl:187-188] - validates shape compatibility
          -> _broadcast_preserving_zero_d(+, A, B)  [arraymath.jl:16-25]
             -> sz = mapreduce(size, ..., (A, B)) - finds shape with highest ndims
             -> arrays_sameshape = map(x -> reshape(x, sz), arrays) - reshapes to match
             -> map(+, arrays_sameshape...) - performs addition with linear indexing
      downstream_surfaces:
        - "Any code relying on map-like shape semantics in array +/- operations"
        - "Code mixing 0D, 1D, and higher-dimensional arrays in addition"
      likelihood: "low"
      impact: "low"
    - effect: "Reshape for Array types is very efficient - creates new Array sharing data, not a wrapper"
      mechanism: |
        For Array inputs specifically, reshape(a::Array, dims) [reshapedarray.jl:43-51] creates
        a new Array{T,N} that shares the underlying memory reference:

        function reshape(a::Array{T,M}, dims::NTuple{N,Int}) where {T,N,M}
            len = Core.checked_dims(dims...)
            if len != length(a)
                throw_dmrsa(dims, length(a))
            end
            ref = a.ref
            return $(Expr(:new, :(Array{T,N}), :ref, :dims))
        end

        This means the reshape overhead is minimal - just allocating a small Array header
        that points to the same data. This is much cheaper than creating a ReshapedArray
        wrapper (which is used for non-Array AbstractArrays).
      downstream_surfaces:
        - "Performance-sensitive code using Array arithmetic"
      likelihood: "high"
      impact: "low"
    - effect: "Method dispatch hierarchy: Array vs AbstractArray follow different paths"
      mechanism: |
        The method dispatch creates a clear hierarchy:

        1. For Array{<:Any,N} + Array{<:Any,N} (same ndims):
           -> _broadcast_preserving_zero_d at line 12-14 -> direct map (fastest)

        2. For Array + Array (different ndims):
           -> _broadcast_preserving_zero_d at line 16-25 -> reshape then map

        3. For AbstractArray + AbstractArray (non-Array types):
           -> _broadcast_preserving_zero_d at line 5-7 -> broadcast_preserving_zero_d
           -> Uses full broadcast semantics (no map optimization)

        The map optimization is only applied to concrete Array types, not AbstractArrays.
      downstream_surfaces:
        - "Custom AbstractArray subtypes still use broadcast (unaffected by this PR)"
        - "StaticArrays, OffsetArrays, etc. maintain their existing behavior"
      likelihood: "high"
      impact: "low"
    - effect: "Multi-argument addition only - no multi-argument subtraction method"
      mechanism: |
        The PR only defines +(A::Array, B::Array, Cs::Array...) for multi-argument addition.
        There is no corresponding -(A::Array, B::Array, Cs::Array...) method because:

        1. Subtraction is not associative: (A - B) - C != A - (B - C)
        2. Julia evaluates A - B - C as (A - B) - C using the two-argument method
        3. The two-argument method at lines 36-39 handles subtraction correctly:

           for f in (:+, :-)
               @eval function ($f)(A::AbstractArray, B::AbstractArray)
                   promote_shape(A, B)
                   _broadcast_preserving_zero_d($f, A, B)
               end
           end

        This is intentional design, not a missing feature.
      downstream_surfaces:
        - "Code using chained subtraction like A - B - C"
      likelihood: "high"
      impact: "low"
    - effect: "promote_shape validates compatibility but does not determine output shape"
      mechanism: |
        promote_shape(a::Dims, b::Dims) [indices.jl:174-185] returns the LONGER shape tuple,
        but this return value is NOT used by the arithmetic operations. Instead:

        function promote_shape(a::Dims, b::Dims)
            if length(a) < length(b)
                return promote_shape(b, a)  # Swap to ensure a is longer
            end
            for i=1:length(b)
                a[i] != b[i] && throw_promote_shape_mismatch(a, b, i)
            end
            for i=length(b)+1:length(a)
                a[i] != 1 && throw_promote_shape_mismatch(a, nothing, i)
            end
            return a  # Returns longer shape
        end

        The arithmetic methods call promote_shape for validation only, then independently
        compute the output shape using mapreduce on the actual array sizes. This ensures
        correct handling even if promote_shape behavior changes.
      downstream_surfaces:
        - "Code that overrides promote_shape for custom arrays"
      likelihood: "low"
      impact: "low"
  compatibility:
    internal_api:
      - field: "_broadcast_preserving_zero_d"
        change: |
          New method signatures added:
          - _broadcast_preserving_zero_d(f, A::Array{<:Any,N}, B::Array{<:Any,N}, Cs::Array{<:Any,N}...) where {N}
          - _broadcast_preserving_zero_d(f, A::Array, B::Array, Cs::Array...)
          These are internal functions not exported from Base.
        affected_tools: []
      - field: "Method signature change for +"
        change: |
          Changed from: +(A::Array, Bs::Array...)
          Changed to:   +(A::Array, B::Array, Cs::Array...)

          The new signature requires at least two Array arguments explicitly, then optional
          additional Arrays. This is more specific than the previous variadic-from-start
          signature. Most user code is unaffected since + is typically called with two args.
        affected_tools: []
    behavioral:
      - description: "Array addition/subtraction now preserves broadcast-like shape semantics for trailing singleton dimensions"
        before: |
          # With PR #59961 only (broken):
          julia> ones(1) + ones(1,1) |> size
          (1,)  # map semantics - drops trailing singleton
        after: |
          # With this fix:
          julia> ones(1) + ones(1,1) |> size
          (1, 1)  # broadcast semantics - preserves trailing singleton
        migration_notes: |
          This is a bugfix restoring expected behavior. Code that relied on the
          broken map-like behavior from PR #59961 should be updated.
  performance:
    compile_time:
      - description: "Negligible compile-time impact"
        estimated_impact: "ESTIMATED: <1% change"
        rationale: |
          The additional method definitions and reshape logic add minimal
          compilation overhead. The hot path for same-ndims arrays
          compiles to the same efficient map call as before.
    runtime:
      - description: "Fast path when all arrays have matching ndims - no performance change"
        estimated_impact: "MEASURED: Equivalent to PR #59961 performance"
        rationale: |
          From PR description: "The alternate approach suggested in #59961 is to reshape
          the arrays, but this adds overhead that nullifies the performance improvement
          for small arrays." The same-ndims fast path avoids this overhead.
      - description: "Reshape overhead when arrays have different ndims - minimal for Array type"
        estimated_impact: "ESTIMATED: <5% overhead for Arrays, 10-30% for AbstractArrays"
        rationale: |
          For Array types specifically, reshape creates a new Array header sharing the
          underlying data [reshapedarray.jl:43-51]. This is O(1) with minimal allocation.

          The actual reshape call:
            arrays_sameshape = map(x -> length(sz) == ndims(x) ? x : reshape(x, sz), arrays)

          Also includes an optimization to skip reshaping when not needed (length(sz) == ndims(x)).

          For non-Array AbstractArrays, the fallback uses broadcast_preserving_zero_d which
          does not use the map optimization at all, so they are unaffected.
      - description: "Multi-argument array addition A + B + C + ... uses single reshape pass"
        estimated_impact: "ESTIMATED: Better than repeated pairwise addition for mixed-ndims"
        rationale: |
          For expressions like A + B + C where all are Arrays with different ndims:
          - With this PR: Single call to _broadcast_preserving_zero_d(+, A, B, C)
            reshapes all arrays once, then performs a single map over all
          - Alternative: Pairwise ((A + B) + C) would require intermediate allocations
  risk:
    level: "low"
    rationale:
      - "This is a targeted bugfix for a regression introduced in PR #59961"
      - "The fix adds method overloads without changing existing behavior for same-ndims arrays"
      - "Comprehensive tests added covering 0D, 1D, and 3D array combinations"
      - "The author (jishnub) also authored PR #59961, understands the context well"
      - "No changes to compiler internals, type inference, or code generation"
      - "For Array types, reshape is very cheap (data sharing, not copying)"
      - "Non-Array AbstractArrays are unaffected - they still use broadcast"
  open_questions:
    - "Whether the reshape overhead for mixed-ndims cases could be further optimized in the future"
    - "Whether similar issues exist in other array operations that were optimized with map"
    - "Whether a multi-argument subtraction method would be useful for -(A, B, Cs...) = A - B - C1 - C2 - ..."
  recommendations:
    - "Downstream packages should verify array arithmetic with mixed dimensions still produces expected shapes"
    - "Performance-sensitive code mixing array dimensions should benchmark after Julia upgrade"
    - "The test coverage added in this PR provides a good regression test pattern for similar optimizations"
    - "Custom AbstractArray subtypes are unaffected - only concrete Array benefits from map optimization"

changelog:
  category: "Bugfix"
  breaking: false
  summary: |
    Fixed array addition/subtraction to preserve broadcast-like shape semantics when
    arrays have different numbers of dimensions. Previously, after PR #59961, adding
    arrays like ones(1) + ones(1,1) would incorrectly return a 1D array instead of
    preserving the 2D shape with trailing singleton dimension.
  for_package_maintainers: |
    This is a bugfix that restores expected behavior. If your package performs array
    arithmetic with mixed-dimensional arrays (e.g., 0D + 1D, 1D + 2D), verify that
    results have the expected shapes after upgrading. The fix ensures output shapes
    match what broadcast would produce.

    Key points:
    - Only affects concrete Array types (not custom AbstractArrays)
    - Same-ndims array operations are unchanged (fast path)
    - Different-ndims operations now reshape to match highest dimensionality
    - For Array types, reshape is very efficient (shares data, no copy)
